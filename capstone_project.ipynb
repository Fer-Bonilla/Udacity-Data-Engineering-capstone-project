{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# BITCOIN AND ETHEREUM DATA CONSOLIDATION\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This project defines the pipeline to load historical data of Bitcoin and Ethereum blockchains and create a Data Lake. The process includes data formatting, cleaning, and transformation. The data lake can be used to analyze ETH and BTC price changes and correlation over time. The main use case for the data can e run prediction and regression models to identify trends and future prices and also can provide advanced visualizations and technical analysis for users using the data. One of the questions can be There is some correlation between BTC and ETH price variation anytime?\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import boto3\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import glob\n",
    "import configparser\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, to_date, to_timestamp, isnan, when, count\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, minute\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Configure goblal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configure java an hadoop global variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read configiguration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# NOTE: Use these if using AWS S3 as a storage\n",
    "INPUT_DATA_AWS                = config['AWS']['INPUT_DATA_AWS']\n",
    "OUTPUT_DATA_AWS               = config['AWS']['OUTPUT_DATA_AWS']\n",
    "\n",
    "# NOTE: Use these if using local storage\n",
    "INPUT_DATA_LOCAL              = config['LOCAL']['INPUT_DATA_LOCAL']\n",
    "OUTPUT_DATA_LOCAL             = config['LOCAL']['OUTPUT_DATA_LOCAL']\n",
    "\n",
    "# Common configuration parameters\n",
    "DATA_LOCATION                 = config['COMMON']['DATA_LOCATION']\n",
    "DATA_STORAGE                  = config['COMMON']['DATA_STORAGE']\n",
    "INPUT_DATA_BTC_DIRECTORY      = config['COMMON']['INPUT_DATA_BTC_DIRECTORY']\n",
    "INPUT_DATA_BTC_ZIP_FILENAME   = config['COMMON']['INPUT_DATA_BTC_ZIP_FILENAME']\n",
    "INPUT_DATA_BTC_FILENAME       = config['COMMON']['INPUT_DATA_BTC_FILENAME']\n",
    "INPUT_DATA_ETH_DIRECTORY      = config['COMMON']['INPUT_DATA_ETH_DIRECTORY']\n",
    "INPUT_DATA_ETH_ZIP_FILENAME   = config['COMMON']['INPUT_DATA_ETH_ZIP_FILENAME']\n",
    "INPUT_DATA_ETH_FILENAME       = config['COMMON']['INPUT_DATA_ETH_FILENAME']\n",
    "OUTPUT_DATA_BTC_FILENAME      = config['COMMON']['OUTPUT_DATA_BTC_FILENAME']\n",
    "OUTPUT_DATA_ETH_FILENAME      = config['COMMON']['OUTPUT_DATA_ETH_FILENAME']\n",
    "OUTPUT_BTC_TABLE_FILENAME     = config['COMMON']['OUTPUT_BTC_TABLE_FILENAME']\n",
    "OUTPUT_ETH_TABLE_FILENAME     = config['COMMON']['OUTPUT_ETH_TABLE_FILENAME']\n",
    "OUTPUT_CRYPTO_TABLE_FILENAME  = config['COMMON']['OUTPUT_CRYPTO_TABLE_FILENAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set global configuration variables\n",
    "if DATA_LOCATION == \"local\":\n",
    "    input_data          = INPUT_DATA_LOCAL\n",
    "    output_data         = OUTPUT_DATA_LOCAL\n",
    "\n",
    "elif DATA_LOCATION == \"aws\":\n",
    "    input_data          = INPUT_DATA_AWS\n",
    "    output_data         = OUTPUT_DATA_AWS\n",
    "    \n",
    "elif DATA_STORAGE == \"parquet\":\n",
    "    data_storage        = DATA_STORAGE\n",
    "    \n",
    "# load variables for BTC data\n",
    "btc_data_directory      = INPUT_DATA_BTC_DIRECTORY\n",
    "btc_zip_filename        = INPUT_DATA_BTC_ZIP_FILENAME    \n",
    "btc_filename            = INPUT_DATA_BTC_FILENAME\n",
    "btc_table_filename      = OUTPUT_BTC_TABLE_FILENAME\n",
    "\n",
    "# load variables for ETH data\n",
    "eth_data_directory      = INPUT_DATA_ETH_DIRECTORY\n",
    "eth_zip_filename        = INPUT_DATA_ETH_ZIP_FILENAME    \n",
    "eth_filename            = INPUT_DATA_ETH_FILENAME\n",
    "eth_table_filename      = OUTPUT_ETH_TABLE_FILENAME\n",
    "\n",
    "# general variables\n",
    "crypto_timeseries_table = OUTPUT_CRYPTO_TABLE_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Unzip data. RUN ONLY if files are compressed. ONLY WORKS FOR LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create btc_data directory\n",
    "#!mkdir btc_data_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create btc_data directory\n",
    "#!unzip btc_zip_filename -d btc_data_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create eth_data directory\n",
    "#!mkdir eth_data_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create btc_data directory\n",
    "#!unzip eth_data_directory -d eth_data_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Project Scope \n",
    "\n",
    "Create a Data Pipeline to process Bitcoin and Ethereum daily prices from CSV files and add to the Data Warehouse, and then can be used to run price prediction models using Machine Learning Time series analysis. The pipeline includes data loading, cleaning, transformation, and aggregation to make the data available to train the ML models. In this project, the main goal is to build the data pipeline to load data to the Data Warehouse and make it available to run ML Models for price prediction.\n",
    "\n",
    "To build the ETL Pipeline Apache Spark On AWS Services is used, and pandas and matplotlib is used to execute the EDA. Pyspark API is used to interact with Spark.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "Datasets used is obtained from Kaggle's datasets, from these repositories:\n",
    "\n",
    "**Bitcoin Historical Data**\n",
    "\n",
    "    * Source: https://www.kaggle.com/mczielinski/bitcoin-historical-data\n",
    "    * Description: Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to March 2021\n",
    "    * Format: Unique CSV file\n",
    "    * Fields: - Timestamp\n",
    "              - Open\n",
    "              - High\n",
    "              - Low\n",
    "              - Close\n",
    "              - Volume_(BTC)\n",
    "              - Volume_(Currency)\n",
    "              - Weighted_Price\n",
    "    * Time period: 2012-01-01 to 2021-3-31\n",
    "\n",
    "**Ethereum (ETH/USDT) 1m Dataset**\n",
    "\n",
    "    * Source: https://www.kaggle.com/priteshkeleven/ethereum-ethusdt-1m-dataset\n",
    "    * Description: Ethereum dataset with 1 minute interval from 17-8-2017 to 03-2-2021\n",
    "    * Format: CSV for each month\n",
    "    * Fields: - timestamp\n",
    "              - open\n",
    "              - high\n",
    "              - low\n",
    "              - close\n",
    "              - volume\n",
    "              - close_time\n",
    "              - quote_av\n",
    "              - trades\n",
    "              - tb_base_av\n",
    "              - tb_quote_av\n",
    "              - ignore\n",
    "\n",
    "    * Time period: 17-8-2017 to 03-2-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.1 Read Bitcoin data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**NOTE**: Original content was compressed zip file. The files were unziped and copu to a local directory to execute the EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/btc_data/*.csv\n"
     ]
    }
   ],
   "source": [
    "# Define a function to read the BTC data from files and consolidate a unique datafram\n",
    "path = os.path.join(input_data,'btc_data' ,'*.csv')\n",
    "print(path)\n",
    "files = glob.glob(path)\n",
    "l_data = []\n",
    "\n",
    "for filename in files:\n",
    "    e_data = pd.read_csv(filename, index_col=None, header=0)\n",
    "    l_data.append(e_data)\n",
    "\n",
    "BTC_data = pd.concat(l_data, axis=0, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325317920</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325317980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1325318040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325318100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1325318160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp  Open  High   Low  Close  Volume_(BTC)  Volume_(Currency)  \\\n",
       "0  1325317920  4.39  4.39  4.39   4.39      0.455581                2.0   \n",
       "1  1325317980   NaN   NaN   NaN    NaN           NaN                NaN   \n",
       "2  1325318040   NaN   NaN   NaN    NaN           NaN                NaN   \n",
       "3  1325318100   NaN   NaN   NaN    NaN           NaN                NaN   \n",
       "4  1325318160   NaN   NaN   NaN    NaN           NaN                NaN   \n",
       "\n",
       "   Weighted_Price  \n",
       "0            4.39  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTC_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume_(BTC)',\n",
       "       'Volume_(Currency)', 'Weighted_Price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data columns\n",
    "BTC_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4857377, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count registers\n",
    "BTC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp                  0\n",
       "Open                 1243608\n",
       "High                 1243608\n",
       "Low                  1243608\n",
       "Close                1243608\n",
       "Volume_(BTC)         1243608\n",
       "Volume_(Currency)    1243608\n",
       "Weighted_Price       1243608\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the null values\n",
    "BTC_data.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.2 Read ETH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function to read the ETH data from files and consolidate a unique datafram\n",
    "path = os.path.join(eth_data_directory, '*.csv')\n",
    "files = glob.glob(path)\n",
    "l_data = []\n",
    "\n",
    "for filename in files:\n",
    "    e_data = pd.read_csv(filename, index_col=None, header=0)\n",
    "    l_data.append(e_data)\n",
    "\n",
    "ETH_data = pd.concat(l_data, axis=0, ignore_index=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_time</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "      <th>ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-19 00:00:00</td>\n",
       "      <td>421.92</td>\n",
       "      <td>423.56</td>\n",
       "      <td>421.51</td>\n",
       "      <td>423.55</td>\n",
       "      <td>1632.38697</td>\n",
       "      <td>1597795259999</td>\n",
       "      <td>689035.133220</td>\n",
       "      <td>511</td>\n",
       "      <td>998.48569</td>\n",
       "      <td>421432.152640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-19 00:01:00</td>\n",
       "      <td>423.56</td>\n",
       "      <td>424.27</td>\n",
       "      <td>423.56</td>\n",
       "      <td>423.98</td>\n",
       "      <td>909.17074</td>\n",
       "      <td>1597795319999</td>\n",
       "      <td>385519.232088</td>\n",
       "      <td>382</td>\n",
       "      <td>352.17966</td>\n",
       "      <td>149320.871212</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-19 00:02:00</td>\n",
       "      <td>424.00</td>\n",
       "      <td>424.00</td>\n",
       "      <td>422.96</td>\n",
       "      <td>423.01</td>\n",
       "      <td>712.07169</td>\n",
       "      <td>1597795379999</td>\n",
       "      <td>301541.453675</td>\n",
       "      <td>305</td>\n",
       "      <td>174.70524</td>\n",
       "      <td>74014.364972</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-19 00:03:00</td>\n",
       "      <td>423.00</td>\n",
       "      <td>423.02</td>\n",
       "      <td>422.56</td>\n",
       "      <td>422.68</td>\n",
       "      <td>680.10097</td>\n",
       "      <td>1597795439999</td>\n",
       "      <td>287561.429434</td>\n",
       "      <td>264</td>\n",
       "      <td>228.55088</td>\n",
       "      <td>96630.824900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-19 00:04:00</td>\n",
       "      <td>422.67</td>\n",
       "      <td>423.07</td>\n",
       "      <td>422.42</td>\n",
       "      <td>422.54</td>\n",
       "      <td>414.13931</td>\n",
       "      <td>1597795499999</td>\n",
       "      <td>175058.889421</td>\n",
       "      <td>315</td>\n",
       "      <td>153.82495</td>\n",
       "      <td>65014.349621</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp    open    high     low   close      volume  \\\n",
       "0  2020-08-19 00:00:00  421.92  423.56  421.51  423.55  1632.38697   \n",
       "1  2020-08-19 00:01:00  423.56  424.27  423.56  423.98   909.17074   \n",
       "2  2020-08-19 00:02:00  424.00  424.00  422.96  423.01   712.07169   \n",
       "3  2020-08-19 00:03:00  423.00  423.02  422.56  422.68   680.10097   \n",
       "4  2020-08-19 00:04:00  422.67  423.07  422.42  422.54   414.13931   \n",
       "\n",
       "      close_time       quote_av  trades  tb_base_av    tb_quote_av  ignore  \n",
       "0  1597795259999  689035.133220     511   998.48569  421432.152640     0.0  \n",
       "1  1597795319999  385519.232088     382   352.17966  149320.871212     0.0  \n",
       "2  1597795379999  301541.453675     305   174.70524   74014.364972     0.0  \n",
       "3  1597795439999  287561.429434     264   228.55088   96630.824900     0.0  \n",
       "4  1597795499999  175058.889421     315   153.82495   65014.349621     0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ETH_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time',\n",
       "       'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data columns\n",
    "ETH_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1817149, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count registers\n",
    "ETH_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp      0\n",
       "open           0\n",
       "high           0\n",
       "low            0\n",
       "close          0\n",
       "volume         0\n",
       "close_time     0\n",
       "quote_av       0\n",
       "trades         0\n",
       "tb_base_av     0\n",
       "tb_quote_av    0\n",
       "ignore         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the null values\n",
    "ETH_data.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.3 Data Quality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### After verify the BTC and ETH data, these quality issues were identified:\n",
    "\n",
    "**istorical BTC data:**\n",
    "\n",
    "  - From 4.857.377 registers in the file, 1.243.608 have null values\n",
    "  - The time format is timestamp and needs transformation to make possible join data to ETH dataset\n",
    "\n",
    "\n",
    "**Historical ETH data:**\n",
    "\n",
    "   - From 1.817.149 registers in the files there are not null values\n",
    "   - The time format is timestamp and needs transformation to make possible join data to ETH dataset\n",
    "\n",
    "- Number of columns between datasets arr diferent, need to map atribbuites.\n",
    "- Registers are not ordered, for the combined table need sorting.\n",
    "\n",
    "\n",
    "**Mapping data fields**\n",
    "\n",
    "    BTC fields                         ETF fields\n",
    "    - Timestamp                        - timestamp \n",
    "    - Open                             - open \n",
    "    - High                             - high \n",
    "    - Low                              - low \n",
    "    - Close                            - close \n",
    "    - Volume_(BTC)                     - close_time \n",
    "    - Volume_(Currency)                - quote_av \n",
    "    - Weighted_Price                   - trades \n",
    "                                       - tb_base_av \n",
    "                                       - tb_quote_av \n",
    "                                       - ignore   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.4 Data cleaning required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Input data requires execute this process:\n",
    "\n",
    "    - Drop null values from staging tables for BTC data (Can't replace with cero values )\n",
    "    - Only use fields that appear in both datasets\n",
    "    - Timestamp need to be splitted into year, month, day, hour \n",
    "    - Data need to be ordered vy date keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 3. Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.1 Conceptual Data Model\n",
    "\n",
    "\n",
    "![conceptual mode](./conceptual_model-spark.png)\n",
    "\n",
    "\n",
    "The basic model is consolidate table that works as the source for ML Models training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    *Table: staging_btc\n",
    "\n",
    "    *Columns:\n",
    "        - Timestamp\n",
    "        - Open\n",
    "        - High\n",
    "        - Low\n",
    "        - Close\n",
    "        - Volume_btc\n",
    "        - Volume_currency\n",
    "        - Weighted_price\n",
    "\n",
    "\n",
    "    *Table: staging_eth\n",
    "\n",
    "    *Columns:\n",
    "        - timestamp\n",
    "        - open\n",
    "        - high\n",
    "        - close\n",
    "        - volume\n",
    "        - close_time\n",
    "        - quote_av\n",
    "        - trades\n",
    "        - tb_base_av\n",
    "        - tb_quote_Av\n",
    "        - ignore\n",
    "\n",
    "\n",
    "    *Table: btc_timeseries\n",
    "\n",
    "    *Columns:\n",
    "        - timestamp\n",
    "        - year\n",
    "        - month\n",
    "        - day\n",
    "        - hour\n",
    "        - btc_open\n",
    "        - btc_high\n",
    "        - btc_low\n",
    "        - btc_close\n",
    "        - btc_volume\n",
    "\n",
    "\n",
    "    *Table: eth_timeseries\n",
    "\n",
    "    *Columns:\n",
    "        - timestamp\n",
    "        - year (Partition Key)\n",
    "        - month\n",
    "        - day\n",
    "        - hour\n",
    "        - eth_open\n",
    "        - eth_high\n",
    "        - eth_low\n",
    "        - eth_close\n",
    "        - eth_volume\n",
    "\n",
    "\n",
    "    *Table: crypto_timeseries\n",
    "\n",
    "    *Columns:\n",
    "        - year (Partition Key)\n",
    "        - month\n",
    "        - day\n",
    "        - hour\n",
    "        - btc_open\n",
    "        - btc_high\n",
    "        - btc_low\n",
    "        - btc_close\n",
    "        - btc_volume\n",
    "        - eth_open\n",
    "        - eth_high\n",
    "        - eth_low\n",
    "        - eth_close\n",
    "        - eth_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "  - Define the global variables in the configuration file (dl.cfg)\n",
    "  - Read data from CSV files from INPUT_FILE Directory into Spark dataframe\n",
    "  - Save Spark dataframes to staging parquet file \n",
    "  - Read BTC parket file and drop null values.\n",
    "  - Transform BTC timestamp in to (year, month, day, hour)\n",
    "  - Transform ETH timestamp in to (year, month, day, hour)\n",
    "  - Join BTC and ETH spark dataframes\n",
    "  - Save data to table crypto_timeseries\n",
    "  - Run the Quality control check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.1 Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.2 Read data from CSV files and write Spark DataFrames to parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the BTC Data Staging file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_(BTC): double (nullable = true)\n",
      " |-- Volume_(Currency): double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      "\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "|Timestamp |Open|High|Low |Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "|1325317920|4.39|4.39|4.39|4.39 |0.45558087  |2.0000000193     |4.39          |\n",
      "|1325317980|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
      "|1325318040|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
      "|1325318100|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
      "|1325318160|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read BTC data to spark\n",
    "btc_data_staging = spark.read.options(header='True', inferSchema='True').csv(INPUT_DATA_BTC_DIRECTORY)\n",
    "btc_data_staging.printSchema()\n",
    "btc_data_staging.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4857377"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BTC Rows Count \n",
    "btc_data_staging.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_BTC: double (nullable = true)\n",
      " |-- Volume_Currency: double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns with not allowed symbols and write the parket file\n",
    "btc_data_staging_temp = btc_data_staging.withColumnRenamed(\"Volume_(BTC)\", \"Volume_BTC\") \\\n",
    "                                        .withColumnRenamed(\"Volume_(Currency)\", \"Volume_Currency\")\n",
    "\n",
    "btc_data_staging_temp.printSchema()\n",
    "btc_data_staging_temp.write.mode(\"overwrite\").parquet(output_data+\"btcstaging.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the ETH Data Staging file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_av: double (nullable = true)\n",
      " |-- trades: integer (nullable = true)\n",
      " |-- tb_base_av: double (nullable = true)\n",
      " |-- tb_quote_av: double (nullable = true)\n",
      " |-- ignore: double (nullable = true)\n",
      "\n",
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+\n",
      "|timestamp             |open  |high  |low   |close |volume  |close_time   |quote_av     |trades|tb_base_av|tb_quote_av  |ignore         |\n",
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+\n",
      "|2017-12-13 00:00:20.81|619.4 |621.99|615.11|615.12|34.17637|1513123280809|21085.5846554|42    |22.16912  |13678.7625208|396902.76727375|\n",
      "|2017-12-13 00:01:20.81|615.12|617.86|615.11|616.99|26.96652|1513123340809|16610.1922788|26    |18.68354  |11509.242068 |396673.21816384|\n",
      "|2017-12-13 00:02:20.81|617.0 |617.0 |613.01|613.01|29.32637|1513123400809|18043.0303292|52    |18.03304  |11102.5676661|396552.02389462|\n",
      "|2017-12-13 00:03:20.81|613.01|615.11|610.0 |611.76|53.86512|1513123460809|32970.7225753|60    |23.70823  |14534.62275  |396556.05743226|\n",
      "|2017-12-13 00:04:20.81|610.8 |612.0 |608.99|608.99|51.15932|1513123520809|31245.7210752|46    |24.61767  |15039.4416256|396537.43108709|\n",
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read ETH data to spark\n",
    "eth_data_staging = spark.read.options(header='True', inferSchema='True').csv('data/eth_data/eth_data/')\n",
    "eth_data_staging.printSchema()\n",
    "eth_data_staging.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1817149"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ETH Rows Count \n",
    "eth_data_staging.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the parket file\n",
    "eth_data_staging.write.mode(\"overwrite\").parquet(output_data+\"ethstaging.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.3 Clean null values from staging BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+----------+---------------+--------------+\n",
      "|Timestamp |Open  |High  |Low   |Close |Volume_BTC|Volume_Currency|Weighted_Price|\n",
      "+----------+------+------+------+------+----------+---------------+--------------+\n",
      "|1471105200|583.73|583.73|583.72|583.72|3.0       |1751.1603323   |583.72011078  |\n",
      "|1471105320|583.81|583.81|583.8 |583.8 |0.2265    |132.231065     |583.80161148  |\n",
      "|1471105680|584.12|584.17|584.12|584.17|0.4511    |263.51742527   |584.16631628  |\n",
      "|1471105860|584.16|584.16|583.82|583.82|2.04297896|1193.3083659   |584.10213186  |\n",
      "|1471105920|584.08|584.08|584.08|584.08|0.0421    |24.589768      |584.08        |\n",
      "+----------+------+------+------+------+----------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3613769"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read parquet file for BTC data and drop null values\n",
    "btc_data_staging = spark.read.parquet(output_data+\"btcstaging.parquet\")\n",
    "btc_data_staging_temp = btc_data_staging.na.drop()\n",
    "btc_data_staging_temp.show(5, truncate=False)\n",
    "btc_data_staging_temp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.4 Format BTC date , create btc_timeseries table and save parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "btc_data_staging_temp = btc_data_staging_temp.withColumn('year',year(to_timestamp('Timestamp')))\n",
    "btc_data_staging_temp = btc_data_staging_temp.withColumn('month',month(to_timestamp('Timestamp')))\n",
    "btc_data_staging_temp = btc_data_staging_temp.withColumn('day',dayofmonth(to_timestamp('Timestamp')))\n",
    "btc_data_staging_temp = btc_data_staging_temp.withColumn('hour',hour(to_timestamp('Timestamp')))\n",
    "btc_data_staging_temp = btc_data_staging_temp.withColumn('minute',minute(to_timestamp('Timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+----------+---------------+--------------+----+-----+---+----+------+\n",
      "|Timestamp |Open  |High  |Low   |Close |Volume_BTC|Volume_Currency|Weighted_Price|year|month|day|hour|minute|\n",
      "+----------+------+------+------+------+----------+---------------+--------------+----+-----+---+----+------+\n",
      "|1471105200|583.73|583.73|583.72|583.72|3.0       |1751.1603323   |583.72011078  |2016|8    |13 |16  |20    |\n",
      "|1471105320|583.81|583.81|583.8 |583.8 |0.2265    |132.231065     |583.80161148  |2016|8    |13 |16  |22    |\n",
      "|1471105680|584.12|584.17|584.12|584.17|0.4511    |263.51742527   |584.16631628  |2016|8    |13 |16  |28    |\n",
      "|1471105860|584.16|584.16|583.82|583.82|2.04297896|1193.3083659   |584.10213186  |2016|8    |13 |16  |31    |\n",
      "|1471105920|584.08|584.08|584.08|584.08|0.0421    |24.589768      |584.08        |2016|8    |13 |16  |32    |\n",
      "+----------+------+------+------+------+----------+---------------+--------------+----+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_data_staging_temp.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- btc_open: double (nullable = true)\n",
      " |-- btc_high: double (nullable = true)\n",
      " |-- btc_low: double (nullable = true)\n",
      " |-- btc_volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# btc_timeseries table creation\n",
    "\n",
    "btc_data_staging_temp.createOrReplaceTempView(\"btc_timeseries\")\n",
    "btc_timeseries_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT Timestamp    AS timestamp,\n",
    "                     year         AS year, \n",
    "                     month        AS month, \n",
    "                     day          AS day, \n",
    "                     hour         AS hour, \n",
    "                     minute       AS minute,\n",
    "                     Open         AS btc_open, \n",
    "                     High         AS btc_high, \n",
    "                     Low          AS btc_low,\n",
    "                     Volume_BTC   AS btc_volume                     \n",
    "    FROM btc_timeseries\n",
    "    ORDER BY year, month, day, hour, minute\n",
    "\"\"\")\n",
    "btc_timeseries_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write btc_timeseries_table to parquet file:\n",
    "btc_timeseries_table.write.mode(\"overwrite\").parquet(output_data+btc_table_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.5 Format ETH date , create eth_timeseries table and save parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+\n",
      "|timestamp             |open  |high  |low   |close |volume  |close_time   |quote_av     |trades|tb_base_av|tb_quote_av  |ignore         |\n",
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+\n",
      "|2017-12-13 00:00:20.81|619.4 |621.99|615.11|615.12|34.17637|1513123280809|21085.5846554|42    |22.16912  |13678.7625208|396902.76727375|\n",
      "|2017-12-13 00:01:20.81|615.12|617.86|615.11|616.99|26.96652|1513123340809|16610.1922788|26    |18.68354  |11509.242068 |396673.21816384|\n",
      "|2017-12-13 00:02:20.81|617.0 |617.0 |613.01|613.01|29.32637|1513123400809|18043.0303292|52    |18.03304  |11102.5676661|396552.02389462|\n",
      "|2017-12-13 00:03:20.81|613.01|615.11|610.0 |611.76|53.86512|1513123460809|32970.7225753|60    |23.70823  |14534.62275  |396556.05743226|\n",
      "|2017-12-13 00:04:20.81|610.8 |612.0 |608.99|608.99|51.15932|1513123520809|31245.7210752|46    |24.61767  |15039.4416256|396537.43108709|\n",
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1817149"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read parquet file for ETH DATA\n",
    "eth_data_staging_temp = spark.read.parquet(output_data+\"ethstaging.parquet\")\n",
    "eth_data_staging_temp.show(5, truncate=False)\n",
    "eth_data_staging_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "eth_data_staging_temp = eth_data_staging_temp.withColumn('year',year(to_timestamp('Timestamp')))\n",
    "eth_data_staging_temp = eth_data_staging_temp.withColumn('month',month(to_timestamp('Timestamp')))\n",
    "eth_data_staging_temp = eth_data_staging_temp.withColumn('day',dayofmonth(to_timestamp('Timestamp')))\n",
    "eth_data_staging_temp = eth_data_staging_temp.withColumn('hour',hour(to_timestamp('Timestamp')))\n",
    "eth_data_staging_temp = eth_data_staging_temp.withColumn('minute',minute(to_timestamp('Timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+----+-----+---+----+------+\n",
      "|timestamp             |open  |high  |low   |close |volume  |close_time   |quote_av     |trades|tb_base_av|tb_quote_av  |ignore         |year|month|day|hour|minute|\n",
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+----+-----+---+----+------+\n",
      "|2017-12-13 00:00:20.81|619.4 |621.99|615.11|615.12|34.17637|1513123280809|21085.5846554|42    |22.16912  |13678.7625208|396902.76727375|2017|12   |13 |0   |0     |\n",
      "|2017-12-13 00:01:20.81|615.12|617.86|615.11|616.99|26.96652|1513123340809|16610.1922788|26    |18.68354  |11509.242068 |396673.21816384|2017|12   |13 |0   |1     |\n",
      "|2017-12-13 00:02:20.81|617.0 |617.0 |613.01|613.01|29.32637|1513123400809|18043.0303292|52    |18.03304  |11102.5676661|396552.02389462|2017|12   |13 |0   |2     |\n",
      "|2017-12-13 00:03:20.81|613.01|615.11|610.0 |611.76|53.86512|1513123460809|32970.7225753|60    |23.70823  |14534.62275  |396556.05743226|2017|12   |13 |0   |3     |\n",
      "|2017-12-13 00:04:20.81|610.8 |612.0 |608.99|608.99|51.15932|1513123520809|31245.7210752|46    |24.61767  |15039.4416256|396537.43108709|2017|12   |13 |0   |4     |\n",
      "+----------------------+------+------+------+------+--------+-------------+-------------+------+----------+-------------+---------------+----+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eth_data_staging_temp.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- eth_open: double (nullable = true)\n",
      " |-- eth_high: double (nullable = true)\n",
      " |-- eth_low: double (nullable = true)\n",
      " |-- eth_volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eth_timeseries table creation\n",
    "\n",
    "eth_data_staging_temp.createOrReplaceTempView(\"eth_timeseries\")\n",
    "eth_timeseries_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT timestamp    AS timestamp,\n",
    "                     year         AS year, \n",
    "                     month        AS month, \n",
    "                     day          AS day, \n",
    "                     hour         AS hour, \n",
    "                     minute       AS minute,\n",
    "                     open         AS eth_open, \n",
    "                     high         AS eth_high, \n",
    "                     low          AS eth_low,\n",
    "                     volume       AS eth_volume                     \n",
    "    FROM eth_timeseries\n",
    "    ORDER BY year, month, day, hour, minute\n",
    "\"\"\")\n",
    "eth_timeseries_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write btc_timeseries_table to parquet file:\n",
    "eth_timeseries_table.write.mode(\"overwrite\").parquet(output_data+eth_table_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Join BTC and ETH data and create crypto_timeseries table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----+------+--------+--------+-------+-----------+\n",
      "|timestamp |year|month|day|hour|minute|btc_open|btc_high|btc_low|btc_volume |\n",
      "+----------+----+-----+---+----+------+--------+--------+-------+-----------+\n",
      "|1528880580|2018|6    |13 |9   |3     |6542.0  |6542.0  |6534.5 |10.04486266|\n",
      "|1528880640|2018|6    |13 |9   |4     |6542.0  |6542.0  |6535.07|1.28037011 |\n",
      "|1528880700|2018|6    |13 |9   |5     |6534.57 |6541.99 |6527.33|30.8100248 |\n",
      "|1528880760|2018|6    |13 |9   |6     |6530.36 |6541.9  |6522.74|5.7461329  |\n",
      "|1528880820|2018|6    |13 |9   |7     |6541.4  |6541.9  |6532.48|0.29524416 |\n",
      "+----------+----+-----+---+----+------+--------+--------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read BTC timeseries table data form parquet file\n",
    "btc_timserie_table = spark.read.parquet(output_data+btc_table_filename)\n",
    "btc_timserie_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+--------+--------+-------+----------+\n",
      "|timestamp          |year|month|day|hour|minute|eth_open|eth_high|eth_low|eth_volume|\n",
      "+-------------------+----+-----+---+----+------+--------+--------+-------+----------+\n",
      "|2021-01-09 02:39:00|2021|1    |9  |2   |39    |1191.2  |1192.31 |1189.73|629.16301 |\n",
      "|2021-01-09 02:40:00|2021|1    |9  |2   |40    |1191.72 |1194.84 |1190.67|617.54553 |\n",
      "|2021-01-09 02:41:00|2021|1    |9  |2   |41    |1194.47 |1194.78 |1189.0 |838.08516 |\n",
      "|2021-01-09 02:42:00|2021|1    |9  |2   |42    |1189.71 |1190.03 |1183.91|1688.38612|\n",
      "|2021-01-09 02:43:00|2021|1    |9  |2   |43    |1184.1  |1186.3  |1183.0 |1020.06746|\n",
      "+-------------------+----+-----+---+----+------+--------+--------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read ETH timeseries table data form parquet file\n",
    "eth_timserie_table = spark.read.parquet(output_data+eth_table_filename)\n",
    "eth_timserie_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join BTC and ETH tables by year, month, day, hour adn minute keys\n",
    "crypto_timeseries_spark = btc_timserie_table.alias('b').join(eth_timserie_table.alias('e'), on=[\n",
    "                                            btc_timserie_table.year ==  eth_timserie_table.year,\n",
    "                                            btc_timserie_table.month ==  eth_timserie_table.month,\n",
    "                                            btc_timserie_table.day ==  eth_timserie_table.day,\n",
    "                                            btc_timserie_table.hour ==  eth_timserie_table.hour,\n",
    "                                            btc_timserie_table.minute ==  eth_timserie_table.minute\n",
    "                                            ]).select(\"b.year\", \n",
    "                                                      \"b.month\",\n",
    "                                                      \"b.day\",\n",
    "                                                      \"b.hour\",\n",
    "                                                      \"b.minute\",\n",
    "                                                      \"b.btc_open\",\n",
    "                                                      \"b.btc_high\",\n",
    "                                                      \"b.btc_low\",\n",
    "                                                      \"b.btc_volume\",                                \n",
    "                                                      \"e.eth_open\",\n",
    "                                                      \"e.eth_high\",\n",
    "                                                      \"e.eth_low\",\n",
    "                                                      \"e.eth_volume\").sort(\"year\",\"month\",\"day\",\"hour\",\"minute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+------+--------+--------+-------+----------+--------+--------+-------+----------+\n",
      "|year|month|day|hour|minute|btc_open|btc_high|btc_low|btc_volume|eth_open|eth_high|eth_low|eth_volume|\n",
      "+----+-----+---+----+------+--------+--------+-------+----------+--------+--------+-------+----------+\n",
      "|2017|8    |17 |4   |0     |4279.35 |4279.35 |4274.0 |1.1698521 |301.13  |301.13  |301.13 |0.42643   |\n",
      "|2017|8    |17 |4   |1     |4270.71 |4270.71 |4270.71|0.09      |301.13  |301.13  |301.13 |2.75787   |\n",
      "|2017|8    |17 |4   |2     |4271.0  |4275.45 |4271.0 |0.58030593|300.0   |300.0   |300.0  |0.0993    |\n",
      "|2017|8    |17 |4   |3     |4274.0  |4274.0  |4271.0 |0.12603432|300.0   |300.0   |300.0  |0.31389   |\n",
      "|2017|8    |17 |4   |4     |4279.35 |4279.35 |4271.0 |0.91548205|301.13  |301.13  |301.13 |0.23202   |\n",
      "+----+-----+---+----+------+--------+--------+-------+----------+--------+--------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crypto_timeseries_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write crypto_timeseries_table to parquet file:\n",
    "crypto_timeseries_spark.write.mode(\"overwrite\").parquet(output_data+crypto_timeseries_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data Quality Checks\n",
    "\n",
    "**Data quality checks:**\n",
    " * Check that all primary and secondary keys in star schema dimension and fact tables have values.\n",
    " * Check that all tables have more than 0 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.1 Quality checks for staging_btc table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(timestamp)|\n",
      "+----------------+\n",
      "|         4857377|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "|count(CASE WHEN (isnan(timestamp) OR (timestamp IS NULL)) THEN timestamp END)|\n",
      "+-----------------------------------------------------------------------------+\n",
      "|                                                                            0|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify table is not empty and key don' have null values\n",
    "btc_data_staging_check = spark.read.parquet(output_data+\"btcstaging.parquet\")\n",
    "btc_data_staging_check.select(count(col('timestamp'))).show()\n",
    "btc_data_staging_check.select(count(when(isnan('timestamp') | col('timestamp').isNull(), 'timestamp'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.2 Quality checks for staging_eth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(timestamp)|\n",
      "+----------------+\n",
      "|         1817149|\n",
      "+----------------+\n",
      "\n",
      "+-------------------------------------------------------+\n",
      "|count(CASE WHEN (timestamp IS NULL) THEN timestamp END)|\n",
      "+-------------------------------------------------------+\n",
      "|                                                      0|\n",
      "+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify table is not empty and key don' have null values\n",
    "eth_data_staging_check = spark.read.parquet(output_data+\"ethstaging.parquet\")\n",
    "eth_data_staging_check.select(count(col('timestamp'))).show()\n",
    "eth_data_staging_check.select(count(when(col('timestamp').isNull(), 'timestamp'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.3 Quality checks for staging_btc table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(timestamp)|\n",
      "+----------------+\n",
      "|         3613769|\n",
      "+----------------+\n",
      "\n",
      "+-------------------------------------------------------+\n",
      "|count(CASE WHEN (timestamp IS NULL) THEN timestamp END)|\n",
      "+-------------------------------------------------------+\n",
      "|                                                      0|\n",
      "+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify table is not empty and key don' have null values\n",
    "btc_data_series_check = spark.read.parquet(output_data+btc_table_filename)\n",
    "btc_data_series_check.select(count(col('timestamp'))).show()\n",
    "btc_data_series_check.select(count(when(col('timestamp').isNull(), 'timestamp'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.4 Quality checks for staging_eth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(timestamp)|\n",
      "+----------------+\n",
      "|         1815899|\n",
      "+----------------+\n",
      "\n",
      "+-------------------------------------------------------+\n",
      "|count(CASE WHEN (timestamp IS NULL) THEN timestamp END)|\n",
      "+-------------------------------------------------------+\n",
      "|                                                      0|\n",
      "+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify table is not empty and key don' have null values\n",
    "eth_data_series_check = spark.read.parquet(output_data+eth_table_filename)\n",
    "eth_data_series_check.select(count(col('timestamp'))).show()\n",
    "eth_data_series_check.select(count(when(col('timestamp').isNull(), 'timestamp'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.5 Quality checks for crypto_timeserie table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(year)|\n",
      "+-----------+\n",
      "|    1767716|\n",
      "+-----------+\n",
      "\n",
      "+---------------------------------------------+\n",
      "|count(CASE WHEN (year IS NULL) THEN year END)|\n",
      "+---------------------------------------------+\n",
      "|                                            0|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify table is not empty and key don' have null values\n",
    "crypto_timeserie_check = spark.read.parquet(output_data+crypto_timeseries_table)\n",
    "crypto_timeserie_check.select(count(col('year'))).show()\n",
    "crypto_timeserie_check.select(count(when(col('year').isNull(), 'year'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "**Table btc_timeseries**\n",
    "\n",
    "|    Field   | Data Type | NULL |                          Description                         |    Source   |\n",
    "|:----------:|:---------:|:----:|:------------------------------------------------------------:|:-----------:|\n",
    "| timestamp  | Integer   | NO   | Timestamp from   original source                             | staging_btc |\n",
    "| year       | Integer   | NO   | Integer   representing the year obtained from the timestamp  | staging_btc |\n",
    "| month      | Integer   | NO   | Integer   representing the month obtained from the timestamp | staging_btc |\n",
    "| day        | Integer   | NO   | Integer   representing the day obtained from the timestamp   | staging_btc |\n",
    "| hour       | Integer   | NO   | Integer   representing the hour obtained from the timestamp  | staging_btc |\n",
    "| btc_open   | double    | NO   | BTC price at   opening                                       | staging_btc |\n",
    "| btc_high   | double    | NO   | Highest BTC price   registered for the period                | staging_btc |\n",
    "| btc_low    | double    | NO   | Lowest BTC price   registered for the period                 | staging_btc |\n",
    "| btc_close  | double    | NO   | BTC Price at time   zone close                               | staging_btc |\n",
    "| btc_volume | double    | NO   | Amount of BTC that   was interchanged                        | staging_btc |\n",
    "\n",
    "\n",
    "**Table eth_timeseries**\n",
    "\n",
    "|    Field   | Data Type | NULL |                          Description                         |    Source   |\n",
    "|:----------:|:---------:|:----:|:------------------------------------------------------------:|:-----------:|\n",
    "| timestamp  | Integer   | NO   | Timestamp from   original source                             | staging_eth |\n",
    "| year       | Integer   | NO   | Integer   representing the year obtained from the timestamp  | staging_eth |\n",
    "| month      | Integer   | NO   | Integer   representing the month obtained from the timestamp | staging_eth |\n",
    "| day        | Integer   | NO   | Integer   representing the day obtained from the timestamp   | staging_eth |\n",
    "| hour       | Integer   | NO   | Integer   representing the hour obtained from the timestamp  | staging_eth |\n",
    "| eth_open   | double    | NO   | ETH price at   opening                                       | staging_eth |\n",
    "| eth_high   | double    | NO   | Highest ETH price   registered for the period                | staging_eth |\n",
    "| eth_low    | double    | NO   | Lowest ETH price   registered for the period                 | staging_eth |\n",
    "| eth_close  | double    | NO   | ETH Price at time   zone close                               | staging_eth |\n",
    "| eth_volume | double    | NO   | Amount of ETH that   was interchanged                        | staging_eth |\n",
    "\n",
    "\n",
    "**Table crypto_timeseries**\n",
    "\n",
    "|    Field   | Data Type | NULL |                          Description                         |     Source     |\n",
    "|:----------:|:---------:|:----:|:------------------------------------------------------------:|:--------------:|\n",
    "| timestamp  | Integer   | NO   | Timestamp from   original source                             | btc_timeseries |\n",
    "| year       | Integer   | NO   | Integer   representing the year obtained from the timestamp  | btc_timeseries |\n",
    "| month      | Integer   | NO   | Integer   representing the month obtained from the timestamp | btc_timeseries |\n",
    "| day        | Integer   | NO   | Integer   representing the day obtained from the timestamp   | btc_timeseries |\n",
    "| hour       | Integer   | NO   | Integer   representing the hour obtained from the timestamp  | btc_timeseries |\n",
    "| btc_open   | double    | NO   | BTC price at   opening                                       | btc_timeseries |\n",
    "| btc_high   | double    | NO   | Highest BTC price   registered for the period                | btc_timeseries |\n",
    "| btc_low    | double    | NO   | Lowest BTC price   registered for the period                 | btc_timeseries |\n",
    "| btc_close  | double    | NO   | BTC Price at time   zone close                               | btc_timeseries |\n",
    "| btc_volume | double    | NO   | Amount of BTC that   was interchanged                        | btc_timeseries |\n",
    "| eth_open   | double    | NO   | ETH price at   opening                                       | eth_timeseries |\n",
    "| eth_high   | double    | NO   | Highest ETH price   registered for the period                | eth_timeseries |\n",
    "| eth_low    | double    | NO   | Lowest ETH price   registered for the period                 | eth_timeseries |\n",
    "| eth_close  | double    | NO   | ETH Price at time   zone close                               | eth_timeseries |\n",
    "| eth_volume | double    | NO   | Amount of ETH that   was interchanged                        | eth_timeseries |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "\n",
    "### 5.1 Rationale for the tools selection:\n",
    "\n",
    "   - Better knowledge developing on Python and Pyspark\n",
    "   - Easy access to run local environment for Pyspark and cloud using AWS Spark clusters.\n",
    "   - PySpark shares some similarities with Python pandas and then is easy to develop with the framework.\n",
    "   - Easy use of the dataframes style data access and SQL \n",
    "   \n",
    "\n",
    "### 5.2 How often ETL script should be run:\n",
    "\n",
    "   - Due to the high dynamic in the cryptocurrency market new info is added each minute, for that reason could be necessary to execute the pipeline at least one time daily. But is not necessary to update all the info only the new one, then the pipeline can be modified to append information, or maybe use a data streaming framework like Kafka can be a better option\n",
    "   \n",
    "\n",
    "### 5.3 Other scenarions (what to consider in them):\n",
    "\n",
    "   - Huge increase in data (Data 100x):  This case can happen if more cryptos are included, which means that each crypto requires more space. In this case, we can use a bigger spark cluster on AWS or another cloud service. Other options include use another database for staging and simplify some columns in the datasets. In that case, we can set up a parallel process on spark to improve the performance\n",
    "   \n",
    "   - The pipelines would be run on a daily basis by 7 am every day: In this case, could be helpful to use a task manager as ML Flow to integrate and schedule the job execution. To update the data every morning is required to integrate the data with providers.\n",
    "\n",
    "   - The database needed to be accessed by 100+ people: For this case, many users can access the data to create specific queries, views, or run their ML models. in that case, a very flexible database is required using the cloud services to provide the data with low latency times.\n",
    "   \n",
    "   \n",
    "### 5.4 Potential further work:    \n",
    "\n",
    "   - Integrate with other data sources as exchanges to integrate more data and more up-to-date price information.\n",
    "   - This approach can require a data streaming service that appends new data\n",
    "   - Integrate with AUTO ML tools or ML flows tools to automatize the prediction models after data is updated.\n",
    "   - Improve the process using MLflow or other data pipelines flow manager.\n",
    "   - Improve the pipeline design and implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
