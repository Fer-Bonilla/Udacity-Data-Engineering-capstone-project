{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BITCOIN AND ETHEREUM FORECASTING\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This project defines the pipeline to load historical data of Bitcoin and Ethereum blockchains and create a Data Lake. The process includes data formatting, cleaning, and transformation. \n",
    "\n",
    "The project follows the follow steps:\n",
    "\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import glob\n",
    "import configparser\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Project Scope \n",
    "\n",
    "Create a Data Pipeline to process Bitcoin and Ethereum daily prices from CSV files and add to the Data Warehouse, and then can be used to run price prediction models using Machine Learning Time series analysis. The pipeline includes data loading, cleaning, transformation, and aggregation to make the data available to train the ML models. In this project, the main goal is to build the data pipeline to load data to the Data Warehouse and make it available to run ML Models for price prediction.\n",
    "\n",
    "To build the ETL Pipeline Apache Spark On AWS Services is used, and pandas and matplotlib is used to execute the EDA. Pyspark API is used to interact with Spark.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "Datasets used is obtained from Kaggle's datasets, from these repositories:\n",
    "\n",
    "**Bitcoin Historical Data**\n",
    "\n",
    "    * Source: https://www.kaggle.com/mczielinski/bitcoin-historical-data\n",
    "    * Description: Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to March 2021\n",
    "    * Format: Unique CSV file\n",
    "    * Fields: - Timestamp\n",
    "              - Open\n",
    "              - High\n",
    "              - Low\n",
    "              - Close\n",
    "              - Volume_(BTC)\n",
    "              - Volume_(Currency)\n",
    "              - Weighted_Price\n",
    "    * Time period: 2012-01-01 to 2021-3-31\n",
    "\n",
    "**Ethereum (ETH/USDT) 1m Dataset**\n",
    "\n",
    "    * Source: https://www.kaggle.com/priteshkeleven/ethereum-ethusdt-1m-dataset\n",
    "    * Description: Ethereum dataset with 1 minute interval from 17-8-2017 to 03-2-2021\n",
    "    * Format: CSV for each month\n",
    "    * Fields: - timestamp\n",
    "              - open\n",
    "              - high\n",
    "              - low\n",
    "              - close\n",
    "              - volume\n",
    "              - close_time\n",
    "              - quote_av\n",
    "              - trades\n",
    "              - tb_base_av\n",
    "              - tb_quote_av\n",
    "              - ignore\n",
    "\n",
    "    * Time period: 17-8-2017 to 03-2-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Bitcoin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gloal directories\n",
    "BTC_DATA_DIR = 'data\\bitcoin_data'\n",
    "ETH_DATA_DIR = 'data\\eth_data'\n",
    "BTC_FILE_DATA = 'bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325317920</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325317980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1325318040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325318100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1325318160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp  Open  High   Low  Close  Volume_(BTC)  Volume_(Currency)  \\\n",
       "0  1325317920  4.39  4.39  4.39   4.39      0.455581                2.0   \n",
       "1  1325317980   NaN   NaN   NaN    NaN           NaN                NaN   \n",
       "2  1325318040   NaN   NaN   NaN    NaN           NaN                NaN   \n",
       "3  1325318100   NaN   NaN   NaN    NaN           NaN                NaN   \n",
       "4  1325318160   NaN   NaN   NaN    NaN           NaN                NaN   \n",
       "\n",
       "   Weighted_Price  \n",
       "0            4.39  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BTC data into Pandas Dataframe\n",
    "BTC_data = pd.read_csv(os.path.join(BTC_DATA_DIR, BTC_FILE_DATA) ,encoding = \"utf-8\")\n",
    "BTC_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume_(BTC)',\n",
       "       'Volume_(Currency)', 'Weighted_Price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data columns\n",
    "BTC_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4857377, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count registers\n",
    "BTC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp                  0\n",
       "Open                 1243608\n",
       "High                 1243608\n",
       "Low                  1243608\n",
       "Close                1243608\n",
       "Volume_(BTC)         1243608\n",
       "Volume_(Currency)    1243608\n",
       "Weighted_Price       1243608\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the null values\n",
    "BTC_data.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ETH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read the ETH data from files and consolidate a unique datafram\n",
    "path = os.path.join(ETH_DATA_DIR, '*.csv')\n",
    "files = glob.glob(path)\n",
    "l_data = []\n",
    "\n",
    "for filename in files:\n",
    "    e_data = pd.read_csv(filename, index_col=None, header=0)\n",
    "    l_data.append(e_data)\n",
    "\n",
    "ETH_data = pd.concat(l_data, axis=0, ignore_index=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_time</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "      <th>ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>733.01</td>\n",
       "      <td>733.97</td>\n",
       "      <td>732.75</td>\n",
       "      <td>732.75</td>\n",
       "      <td>19.77247</td>\n",
       "      <td>1514764859999</td>\n",
       "      <td>14490.961596</td>\n",
       "      <td>29</td>\n",
       "      <td>1.74674</td>\n",
       "      <td>1281.803635</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:01:00</td>\n",
       "      <td>733.34</td>\n",
       "      <td>734.52</td>\n",
       "      <td>732.51</td>\n",
       "      <td>732.51</td>\n",
       "      <td>26.05199</td>\n",
       "      <td>1514764919999</td>\n",
       "      <td>19105.098094</td>\n",
       "      <td>50</td>\n",
       "      <td>21.31950</td>\n",
       "      <td>15638.007363</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:02:00</td>\n",
       "      <td>732.51</td>\n",
       "      <td>734.49</td>\n",
       "      <td>732.49</td>\n",
       "      <td>732.51</td>\n",
       "      <td>15.71883</td>\n",
       "      <td>1514764979999</td>\n",
       "      <td>11515.440038</td>\n",
       "      <td>40</td>\n",
       "      <td>3.12611</td>\n",
       "      <td>2291.043904</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:03:00</td>\n",
       "      <td>732.51</td>\n",
       "      <td>733.83</td>\n",
       "      <td>730.00</td>\n",
       "      <td>730.36</td>\n",
       "      <td>29.43683</td>\n",
       "      <td>1514765039999</td>\n",
       "      <td>21532.608268</td>\n",
       "      <td>59</td>\n",
       "      <td>3.50540</td>\n",
       "      <td>2567.132006</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 00:04:00</td>\n",
       "      <td>730.36</td>\n",
       "      <td>731.00</td>\n",
       "      <td>728.93</td>\n",
       "      <td>728.93</td>\n",
       "      <td>42.50766</td>\n",
       "      <td>1514765099999</td>\n",
       "      <td>31036.170949</td>\n",
       "      <td>103</td>\n",
       "      <td>17.01968</td>\n",
       "      <td>12429.254412</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp    open    high     low   close    volume  \\\n",
       "0  2018-01-01 00:00:00  733.01  733.97  732.75  732.75  19.77247   \n",
       "1  2018-01-01 00:01:00  733.34  734.52  732.51  732.51  26.05199   \n",
       "2  2018-01-01 00:02:00  732.51  734.49  732.49  732.51  15.71883   \n",
       "3  2018-01-01 00:03:00  732.51  733.83  730.00  730.36  29.43683   \n",
       "4  2018-01-01 00:04:00  730.36  731.00  728.93  728.93  42.50766   \n",
       "\n",
       "      close_time      quote_av  trades  tb_base_av   tb_quote_av  ignore  \n",
       "0  1514764859999  14490.961596      29     1.74674   1281.803635     0.0  \n",
       "1  1514764919999  19105.098094      50    21.31950  15638.007363     0.0  \n",
       "2  1514764979999  11515.440038      40     3.12611   2291.043904     0.0  \n",
       "3  1514765039999  21532.608268      59     3.50540   2567.132006     0.0  \n",
       "4  1514765099999  31036.170949     103    17.01968  12429.254412     0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ETH_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time',\n",
       "       'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data columns\n",
    "ETH_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1817149, 12)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count registers\n",
    "ETH_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp      0\n",
       "open           0\n",
       "high           0\n",
       "low            0\n",
       "close          0\n",
       "volume         0\n",
       "close_time     0\n",
       "quote_av       0\n",
       "trades         0\n",
       "tb_base_av     0\n",
       "tb_quote_av    0\n",
       "ignore         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the null values\n",
    "ETH_data.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define config and read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configiguration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# NOTE: Use these if using AWS S3 as a storage\n",
    "INPUT_DATA = config['AWS']['INPUT_DATA']\n",
    "OUTPUT_DATA = config['AWS']['OUTPUT_DATA']\n",
    "\n",
    "# NOTE: Use these if using local storage\n",
    "INPUT_DATA_LOCAL          = config['LOCAL']['INPUT_DATA_LOCAL']\n",
    "INPUT_DATA_BTC_LOCAL      = config['LOCAL']['INPUT_DATA_BTC_LOCAL']\n",
    "INPUT_DATA_ETH_LOCAL      = config['LOCAL']['INPUT_DATA_ETH_LOCAL']\n",
    "OUTPUT_DATA_LOCAL         = config['LOCAL']['OUTPUT_DATA_LOCAL']\n",
    "\n",
    "# NOTE: Use these when storing data on server.\n",
    "INPUT_DATA_BTC_SERVER     = config['SERVER']['INPUT_DATA_BTC_SERVER']\n",
    "INPUT_DATA_ETHT_SERVER    = config['SERVER']['INPUT_DATA_ETH_SERVER']\n",
    "OUTPUT_DATA_SERVER        = config['SERVER']['OUTPUT_DATA_SERVER']\n",
    "\n",
    "# NOTE: Use these when storing data on AWS.\n",
    "INPUT_DATA_BTC            = config['AWS']['INPUT_DATA_BTC']\n",
    "INPUT_DATA_ETH            = config['AWS']['INPUT_DATA_ETH']\n",
    "OUTPUT_DATA               = config['AWS']['OUTPUT_DATA']\n",
    "\n",
    "DATA_LOCATION             = config['COMMON']['DATA_LOCATION']\n",
    "DATA_STORAGE              = config['COMMON']['DATA_STORAGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global configuration variables\n",
    "if DATA_LOCATION == \"local\":\n",
    "    input_data        = INPUT_DATA_LOCAL\n",
    "    btc_data          = INPUT_DATA_BTC_LOCAL\n",
    "    eth_data          = INPUT_DATA_ETH_LOCAL\n",
    "    output_data       = OUTPUT_DATA_LOCAL\n",
    "elif DATA_LOCATION == \"server\":\n",
    "    input_data_bucket = INPUT_DATA_SERVER\n",
    "    i94_data          = INPUT_DATA_BTC_SERVER\n",
    "    airport_codes     = INPUT_DATA_ETH_SERVER\n",
    "    output_data       = OUTPUT_DATA_SERVER\n",
    "elif DATA_LOCATION == \"aws\":\n",
    "    input_data_bucket = INPUT_DATA\n",
    "    i94_data          = INPUT_DATA_BTC\n",
    "    airport_codes     = INPUT_DATA_ETH\n",
    "    output_data       = OUTPUT_DATA\n",
    "    \n",
    "if DATA_STORAGE == \"postgresql\":\n",
    "    pass\n",
    "elif DATA_STORAGE == \"parquet\":\n",
    "    data_storage      = DATA_STORAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-dc3f2cff0a44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m                     \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.jars.packages\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"saurfang:spark-sas7bdat:2.0.0-s_2.11\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m                     \u001b[1;33m.\u001b[0m\u001b[0menableHiveSupport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "                    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Read BITCOIN data to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_schema = t.StructType([\n",
    "                            t.StructField(\"alpha-2\", t.StringType(), False),\n",
    "                            t.StructField(\"alpha-3\", t.StringType(), False),\n",
    "                            t.StructField(\"country-code\", t.IntegerType(), False),\n",
    "                            t.StructField(\"intermediate-region\", t.StringType(), False),\n",
    "                            t.StructField(\"intermediate-region-code\", t.StringType(), False),\n",
    "                            t.StructField(\"iso-3166-2\", t.StringType(), False),\n",
    "                            t.StructField(\"name\", t.StringType(), False),\n",
    "                            t.StructField(\"region\", t.StringType(), True),\n",
    "                            t.StructField(\"region-code\", t.StringType(), True),\n",
    "                            t.StructField(\"sub-region\", t.StringType(), True),\n",
    "                            t.StructField(\"sub-region-code\", t.StringType(), True),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94_df_spark =spark.read.format('com.github.saurfang.sas.spark').load(i94_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "|cicid|i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum      |fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "|7.0  |2016.0|1.0   |101.0 |101.0 |BOS    |20465.0|1.0    |MA     |null   |20.0  |3.0    |1.0  |null    |null    |null |T      |null   |null   |null   |1996.0 |D/S     |M     |null  |LH     |3.46608285E8|424  |F1      |\n",
      "|8.0  |2016.0|1.0   |101.0 |101.0 |BOS    |20465.0|1.0    |MA     |null   |20.0  |3.0    |1.0  |null    |null    |null |T      |null   |null   |null   |1996.0 |D/S     |M     |null  |LH     |3.46627585E8|424  |F1      |\n",
      "|9.0  |2016.0|1.0   |101.0 |101.0 |BOS    |20469.0|1.0    |CT     |20480.0|17.0  |2.0    |1.0  |null    |null    |null |T      |N      |null   |M      |1999.0 |07152016|F     |null  |AF     |3.81092385E8|338  |B2      |\n",
      "|10.0 |2016.0|1.0   |101.0 |101.0 |BOS    |20469.0|1.0    |CT     |20499.0|45.0  |2.0    |1.0  |null    |null    |null |T      |N      |null   |M      |1971.0 |07152016|F     |null  |AF     |3.81087885E8|338  |B2      |\n",
      "|11.0 |2016.0|1.0   |101.0 |101.0 |BOS    |20469.0|1.0    |CT     |20499.0|12.0  |2.0    |1.0  |null    |null    |null |T      |N      |null   |M      |2004.0 |07152016|M     |null  |AF     |3.81078685E8|338  |B2      |\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_df_spark.printSchema()\n",
    "i94_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Read Airport code data to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airport_schema = t.StructType([\n",
    "#                            t.StructField(\"dt\", t.StringType(), False),\n",
    "#                            t.StructField(\"AverageTemperature\", t.FloatType(), True),\n",
    "#                            t.StructField(\"AverageTemperatureUncertainty\", t.FloatType(), True),\n",
    "#                            t.StructField(\"City\", t.StringType(), False),\n",
    "#                            t.StructField(\"Country\", t.StringType(), False),\n",
    "#                            t.StructField(\"Latitude\", t.StringType(), False),\n",
    "#                            t.StructField(\"Longitude\", t.StringType(), False),\n",
    "#                        ])\n",
    "airport_codes_iata_df_spark = spark.read.csv(airport_codes, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+-------------------------------------+\n",
      "|ident|type         |name                              |elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates                          |\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+-------------------------------------+\n",
      "|00A  |heliport     |Total Rf Heliport                 |11          |NA       |US         |US-PA     |Bensalem    |00A     |null     |00A       |-74.93360137939453, 40.07080078125   |\n",
      "|00AA |small_airport|Aero B Ranch Airport              |3435        |NA       |US         |US-KS     |Leoti       |00AA    |null     |00AA      |-101.473911, 38.704022               |\n",
      "|00AK |small_airport|Lowell Field                      |450         |NA       |US         |US-AK     |Anchor Point|00AK    |null     |00AK      |-151.695999146, 59.94919968          |\n",
      "|00AL |small_airport|Epps Airpark                      |820         |NA       |US         |US-AL     |Harvest     |00AL    |null     |00AL      |-86.77030181884766, 34.86479949951172|\n",
      "|00AR |closed       |Newport Hospital & Clinic Heliport|237         |NA       |US         |US-AR     |Newport     |null    |null     |null      |-91.254898, 35.6087                  |\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_iata_df_spark.printSchema()\n",
    "airport_codes_iata_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Write Spark DataFrames to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-21-08-25-59-465542\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/i94_staging.parquet_2019-08-21-08-25-59-465542\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write I94 Immigration data to parquet file:\n",
    "i94_df_path = output_data + \"i94_staging.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {i94_df_path}\")\n",
    "i94_df_spark.write.mode(\"overwrite\").parquet(i94_df_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "i94_df_spark = spark.read.parquet(i94_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i94_df_spark.printSchema()\n",
    "#i94_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/airport_codes_i94_staging.parquet_2019-08-21-08-25-59-465542\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write I94 Airport data to parquet file:\n",
    "airport_codes_i94_df_path = output_data + \"airport_codes_i94_staging.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {airport_codes_i94_df_path}\")\n",
    "airport_codes_i94_df_spark.write.mode(\"overwrite\").parquet(airport_codes_i94_df_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "airport_codes_i94_df_spark = spark.read.parquet(airport_codes_i94_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airport_codes_i94_df_spark.printSchema()\n",
    "#airport_codes_i94_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/country_codes_i94_staging.parquet_2019-08-21-08-25-59-465542\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write i94 Country data to parquet file:\n",
    "country_codes_i94_df_path = output_data + \"country_codes_i94_staging.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {country_codes_i94_df_path}\")\n",
    "country_codes_i94_df_spark.write.mode(\"overwrite\").parquet(country_codes_i94_df_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "country_codes_i94_df_spark = spark.read.parquet(country_codes_i94_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_codes_i94_df_spark.printSchema()\n",
    "#country_codes_i94_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/airport_codes_iata_staging.parquet_2019-08-21-08-25-59-465542\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write IATA Airport data to parquet file:\n",
    "airport_codes_iata_df_path = output_data + \"airport_codes_iata_staging.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {airport_codes_iata_df_path}\")\n",
    "airport_codes_iata_df_spark.write.mode(\"overwrite\").parquet(airport_codes_iata_df_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "airport_codes_iata_df_spark = spark.read.parquet(airport_codes_iata_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airport_codes_iata_df_spark.printSchema()\n",
    "#airport_codes_iata_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/country_codes_iso_staging.parquet_2019-08-21-08-33-50-378418\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write ISO-3166 Country Code data to parquet file:\n",
    "country_codes_iso_df_path = output_data + \"country_codes_iso_staging.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {country_codes_iso_df_path}\")\n",
    "country_codes_iso_df_spark.write.mode(\"overwrite\").parquet(country_codes_iso_df_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "country_code_iso_df_spark = spark.read.parquet(country_codes_iso_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_codes_iso_df_spark.printSchema()\n",
    "#country_codes_iso_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "_Identify data quality issues, like missing values, duplicate data, etc._\n",
    "\n",
    "Input data has the following quality issues:\n",
    "    \n",
    "* I94 Immigration data: \n",
    "    * Most of the columns are missing some info (nulls).\n",
    "    * All missing info need to be filled-in to avoid errors further in the pipeline. \n",
    "    \n",
    "* I94 Airport data: \n",
    "    * Data has quote marks and extra white spaces aftwer copy-paste operation.\n",
    "    * Original data was cleaned-up already before importing to Spark.\n",
    "    \n",
    "* I94 Country code data: \n",
    "    * Data has quote marks and extra white spaces aftwer copy-paste operation.\n",
    "    * Original data was cleaned-up already before importing to Spark.\n",
    "\n",
    "* ISO3166 Country data:\n",
    "    * Antarctica (row) is missing data from some columns.\n",
    "\n",
    "#### Cleaning Steps\n",
    "_Document steps necessary to clean the data_\n",
    "\n",
    "Input data needs the following cleaning operations:\n",
    "* I94 data:\n",
    "    * All missing (null) data is handled in all columns. \n",
    "    * Nulls are replaced with either NA (string), or 0.0 (double).\n",
    "    \n",
    "* I94 Airport data: \n",
    "    * Remove quote marks and extra white spaces from the data.\n",
    "* I94 Country Code data: \n",
    "    * Remove quote marks and extra white spaces from the data.\n",
    "* ISO Country Code data:\n",
    "    * No action required. Antarctica is handled as a special case to avoid duplicate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Clean I94 Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NULLs DONE.\n"
     ]
    }
   ],
   "source": [
    "# Cleaning i94 data\n",
    "i94_df_spark_clean = i94_df_spark.na.fill({'i94mode': 0.0, 'i94addr': 'NA','depdate': 0.0, 'i94bir': 'NA', \\\n",
    "                        'i94visa': 0.0, 'count': 0.0, 'dtadfile': 'NA', 'visapost': 'NA', \\\n",
    "                        'occup': 'NA', 'entdepa': 'NA', 'entdepd': 'NA', 'entdepu': 'NA', \\\n",
    "                        'matflag': 'NA', 'biryear': 0.0, 'dtaddto': 'NA', 'gender': 'NA', \\\n",
    "                        'insnum': 'NA', 'airline': 'NA', 'admnum': 0.0, 'fltno': 'NA', 'visatype': 'NA'})\n",
    "print(\"Filling NULLs DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No cleaning actions. All necessary columns have clean data.\n",
    "#i94_df_spark_clean.createOrReplaceTempView(\"immigrants_table_DF\")\n",
    "#immigrants_table_check = spark.sql(\"\"\"\n",
    "#    SELECT  cicid, i94yr, i94mon, i94cit, i94res, i94port, arrdate, \\\n",
    "#            i94mode, airline, fltno, depdate, i94bir, i94visa, gender,  \\\n",
    "#            visatype, admnum\n",
    "#    FROM immigrants_table_DF\n",
    "#    WHERE   cicid == null OR arrdate == null OR i94port == null \\\n",
    "#            OR fltno == null OR i94mode == null OR admnum == null \\\n",
    "#            OR gender == null OR admnum == null \n",
    "#    ORDER BY arrdate\n",
    "#\"\"\")\n",
    "#immigrants_table_check.printSchema()\n",
    "#immigrants_table_check.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+---------+----------+-------------------+\n",
      "|cicid|i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum      |fltno|visatype|person_id|i94res_str|arrival_ts         |\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+---------+----------+-------------------+\n",
      "|7.0  |2016.0|1.0   |101.0 |101.0 |BOS    |20465.0|1.0    |MA     |0.0    |20.0  |3.0    |1.0  |NA      |NA      |NA   |T      |NA     |NA     |NA     |1996.0 |D/S     |M     |NA    |LH     |3.46608285E8|424  |F1      |0        |101       |2016-01-12 00:00:00|\n",
      "|8.0  |2016.0|1.0   |101.0 |101.0 |BOS    |20465.0|1.0    |MA     |0.0    |20.0  |3.0    |1.0  |NA      |NA      |NA   |T      |NA     |NA     |NA     |1996.0 |D/S     |M     |NA    |LH     |3.46627585E8|424  |F1      |1        |101       |2016-01-12 00:00:00|\n",
      "|9.0  |2016.0|1.0   |101.0 |101.0 |BOS    |20469.0|1.0    |CT     |20480.0|17.0  |2.0    |1.0  |NA      |NA      |NA   |T      |N      |NA     |M      |1999.0 |07152016|F     |NA    |AF     |3.81092385E8|338  |B2      |2        |101       |2016-01-16 00:00:00|\n",
      "|10.0 |2016.0|1.0   |101.0 |101.0 |BOS    |20469.0|1.0    |CT     |20499.0|45.0  |2.0    |1.0  |NA      |NA      |NA   |T      |N      |NA     |M      |1971.0 |07152016|F     |NA    |AF     |3.81087885E8|338  |B2      |3        |101       |2016-01-16 00:00:00|\n",
      "|11.0 |2016.0|1.0   |101.0 |101.0 |BOS    |20469.0|1.0    |CT     |20499.0|12.0  |2.0    |1.0  |NA      |NA      |NA   |T      |N      |NA     |M      |2004.0 |07152016|M     |NA    |AF     |3.81078685E8|338  |B2      |4        |101       |2016-01-16 00:00:00|\n",
      "|12.0 |2016.0|1.0   |101.0 |101.0 |BOS    |20474.0|1.0    |MA     |0.0    |33.0  |2.0    |1.0  |NA      |NA      |NA   |T      |NA     |NA     |NA     |1983.0 |07202016|M     |NA    |LH     |4.06155985E8|424  |B2      |5        |101       |2016-01-21 00:00:00|\n",
      "|15.0 |2016.0|1.0   |101.0 |101.0 |BOS    |20477.0|1.0    |MA     |20524.0|28.0  |3.0    |1.0  |NA      |NA      |NA   |T      |O      |NA     |M      |1988.0 |D/S     |F     |NA    |LH     |4.17363085E8|424  |F1      |6        |101       |2016-01-24 00:00:00|\n",
      "|17.0 |2016.0|1.0   |101.0 |101.0 |BOS    |20480.0|1.0    |MA     |0.0    |78.0  |2.0    |1.0  |NA      |NA      |NA   |T      |NA     |NA     |NA     |1938.0 |07262016|M     |NA    |TK     |4.28558285E8|81   |B2      |7        |101       |2016-01-27 00:00:00|\n",
      "|18.0 |2016.0|1.0   |101.0 |101.0 |BOS    |20480.0|1.0    |MA     |0.0    |70.0  |2.0    |1.0  |NA      |NA      |NA   |T      |NA     |NA     |NA     |1946.0 |07262016|F     |NA    |TK     |4.28561085E8|81   |B2      |8        |101       |2016-01-27 00:00:00|\n",
      "|20.0 |2016.0|1.0   |101.0 |101.0 |CHI    |20473.0|1.0    |IL     |20482.0|28.0  |2.0    |1.0  |NA      |NA      |NA   |T      |O      |NA     |M      |1988.0 |07192016|M     |NA    |BA     |4.01779785E8|295  |B2      |9        |101       |2016-01-20 00:00:00|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+---------+----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#i94_df_spark_clean.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Clean I94 Airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94port_clean</th>\n",
       "      <th>i94_airport_name_clean</th>\n",
       "      <th>i94_state_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DTH</td>\n",
       "      <td>DUTCH HARBOR</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EGL</td>\n",
       "      <td>EAGLE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FRB</td>\n",
       "      <td>FAIRBANKS</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOM</td>\n",
       "      <td>HOMER</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HYD</td>\n",
       "      <td>HYDER</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  i94port_clean    i94_airport_name_clean i94_state_clean\n",
       "0           ALC                     ALCAN              AK\n",
       "1           ANC                 ANCHORAGE              AK\n",
       "2           BAR  BAKER AAF - BAKER ISLAND              AK\n",
       "3           DAC             DALTONS CACHE              AK\n",
       "4           PIZ    DEW STATION PT LAY DEW              AK\n",
       "5           DTH              DUTCH HARBOR              AK\n",
       "6           EGL                     EAGLE              AK\n",
       "7           FRB                 FAIRBANKS              AK\n",
       "8           HOM                     HOMER              AK\n",
       "9           HYD                     HYDER              AK"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No further cleaning required.\n",
    "airport_codes_i94_df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Clean I94 Country Code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94cit_clean</th>\n",
       "      <th>i94_country_name_clean</th>\n",
       "      <th>iso_country_code_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>324</td>\n",
       "      <td>ANGOLA</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>529</td>\n",
       "      <td>ANGUILLA</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>518</td>\n",
       "      <td>ANTIGUA-BARBUDA</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>687</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>151</td>\n",
       "      <td>ARMENIA</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i94cit_clean i94_country_name_clean  iso_country_code_clean\n",
       "0           582                 MEXICO                     484\n",
       "1           236            AFGHANISTAN                       4\n",
       "2           101                ALBANIA                       8\n",
       "3           316                ALGERIA                      12\n",
       "4           102                ANDORRA                      20\n",
       "5           324                 ANGOLA                      24\n",
       "6           529               ANGUILLA                     660\n",
       "7           518        ANTIGUA-BARBUDA                      28\n",
       "8           687              ARGENTINA                      32\n",
       "9           151                ARMENIA                      51"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No further cleaning required.\n",
    "country_codes_i94_df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Clean ISO Country Codes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- alpha_2: string (nullable = false)\n",
      " |-- alpha_3: string (nullable = false)\n",
      " |-- country_code: string (nullable = false)\n",
      " |-- iso_3166_2: string (nullable = false)\n",
      " |-- region: string (nullable = true)\n",
      " |-- sub_region: string (nullable = true)\n",
      " |-- intermediate_region: string (nullable = false)\n",
      " |-- region_code: string (nullable = true)\n",
      " |-- sub_region_code: string (nullable = true)\n",
      " |-- intermediate_region_code: string (nullable = false)\n",
      "\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "|name          |alpha_2|alpha_3|country_code|iso_3166_2   |region |sub_region     |intermediate_region|region_code|sub_region_code|intermediate_region_code|\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "|Afghanistan   |AF     |AFG    |4           |ISO 3166-2:AF|Asia   |Southern Asia  |NaN                |142.0      |34.0           |NaN                     |\n",
      "|land Islands |AX     |ALA    |248         |ISO 3166-2:AX|Europe |Northern Europe|NaN                |150.0      |154.0          |NaN                     |\n",
      "|Albania       |AL     |ALB    |8           |ISO 3166-2:AL|Europe |Southern Europe|NaN                |150.0      |39.0           |NaN                     |\n",
      "|Algeria       |DZ     |DZA    |12          |ISO 3166-2:DZ|Africa |Northern Africa|NaN                |2.0        |15.0           |NaN                     |\n",
      "|American Samoa|AS     |ASM    |16          |ISO 3166-2:AS|Oceania|Polynesia      |NaN                |9.0        |61.0           |NaN                     |\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes_iso_df_spark.printSchema()\n",
    "country_codes_iso_df_spark.show(5, truncate=False)\n",
    "country_codes_iso_df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NULLs DONE.\n"
     ]
    }
   ],
   "source": [
    "# Cleaning ISO-3166 country code data\n",
    "country_codes_iso_df_spark_clean = country_codes_iso_df_spark\\\n",
    "                                        .na.fill({  'name': 'NA', \\\n",
    "                                                    'alpha_2': 'NA', \\\n",
    "                                                    'alpha_3': 'NA', \\\n",
    "                                                    'country_code': 0, \\\n",
    "                                                    'iso_3166_2': 'NA', \\\n",
    "                                                    'region': 'NA', \\\n",
    "                                                    'sub_region': 'NA', \\\n",
    "                                                    'intermediate_region': 'NA', \\\n",
    "                                                    'region_code': 'NA', \\\n",
    "                                                    'sub_region_code': 'NA', \\\n",
    "                                                    'intermediate_region_code': 'NA'})\n",
    "print(\"Filling NULLs DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_codes_iso_df_spark_clean.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "_Map out the conceptual data model and explain why you chose that model_\n",
    "\n",
    "I94 Immigration Insights data models is a star models consisting of 4 Dimensions table and 1 Fact table:\n",
    "  * Dimensions tables:\n",
    "      * admissions table\n",
    "      * countries table\n",
    "      * airports table\n",
    "      * time table\n",
    "  * Fact table:\n",
    "      * immigrations table\n",
    "      \n",
    "ERD for the project:\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "_List the steps necessary to pipeline the data into the chosen data model_\n",
    "* First, ETL script reads in configuration settings (dl.cfg). Script also re-orders I94 inout files to process them in right order (Jan => Dec).\n",
    "* ETL script takes input data (I94 data, I94 country data, I94 airport data, ISO-3166 country data, IATA airport data).\n",
    "* Raw input data is read into pandas dataframe, and from there to Spark dataframe and stored into parquet staging files.\n",
    "* Staging parquet files are read back to Spark dataframes and cleaned (when necessary) and some further data is extracted from the original data.\n",
    "* Each star schema table is processed in order: admissions => countries => airports => time => immigrations\n",
    "* Finally, data quality checks are run for each table to validate the output (key columns don't have nulls, each table has content). A summary of the quality check is provided and written in console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-21-08-33-50-378418\n"
     ]
    }
   ],
   "source": [
    "# Write code here\n",
    "start_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Create admissions table + write to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- admission_nbr: double (nullable = false)\n",
      " |-- country_code: double (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- visa_code: double (nullable = false)\n",
      " |-- visa_type: string (nullable = false)\n",
      " |-- person_gender: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create table\n",
    "#i94_df_spark = i94_df_spark.withColumn(\"person_id\", monotonically_increasing_id())\n",
    "i94_df_spark_clean.createOrReplaceTempView(\"admissions_table_DF\")\n",
    "admissions_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT admnum   AS admission_nbr,\n",
    "                     i94res   AS country_code, \n",
    "                     i94bir   AS age, \n",
    "                     i94visa  AS visa_code, \n",
    "                     visatype AS visa_type, \n",
    "                     gender   AS person_gender\n",
    "    FROM admissions_table_DF\n",
    "    ORDER BY country_code\n",
    "\"\"\")\n",
    "admissions_table.printSchema()\n",
    "#admissions_table.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/admissions_table.parquet_2019-08-21-08-33-50-378418\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write admissions_table to parquet file:\n",
    "admissions_table_path = output_data + \"admissions_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {admissions_table_path}\")\n",
    "admissions_table.write.mode(\"overwrite\").parquet(admissions_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "admissions_table_df = spark.read.parquet(admissions_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Create countries table + write to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94_cit: string (nullable = true)\n",
      " |-- i94_country_name: string (nullable = true)\n",
      " |-- iso_country_code: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+----------------+\n",
      "|i94_cit|    i94_country_name|iso_country_code|\n",
      "+-------+--------------------+----------------+\n",
      "|    527|TURKS AND CAICOS ...|             796|\n",
      "|    420|              TUVALU|             798|\n",
      "|    352|              UGANDA|             800|\n",
      "|    162|             UKRAINE|             804|\n",
      "|    296|UNITED ARAB EMIRATES|             784|\n",
      "+-------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes_i94_df_spark.printSchema()\n",
    "country_codes_i94_df_spark.show(5)\n",
    "country_codes_i94_df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- alpha_2: string (nullable = false)\n",
      " |-- alpha_3: string (nullable = false)\n",
      " |-- country_code: string (nullable = false)\n",
      " |-- iso_3166_2: string (nullable = false)\n",
      " |-- region: string (nullable = true)\n",
      " |-- sub_region: string (nullable = true)\n",
      " |-- intermediate_region: string (nullable = false)\n",
      " |-- region_code: string (nullable = true)\n",
      " |-- sub_region_code: string (nullable = true)\n",
      " |-- intermediate_region_code: string (nullable = false)\n",
      "\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "|          name|alpha_2|alpha_3|country_code|   iso_3166_2| region|     sub_region|intermediate_region|region_code|sub_region_code|intermediate_region_code|\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "|   Afghanistan|     AF|    AFG|           4|ISO 3166-2:AF|   Asia|  Southern Asia|                NaN|      142.0|           34.0|                     NaN|\n",
      "| land Islands|     AX|    ALA|         248|ISO 3166-2:AX| Europe|Northern Europe|                NaN|      150.0|          154.0|                     NaN|\n",
      "|       Albania|     AL|    ALB|           8|ISO 3166-2:AL| Europe|Southern Europe|                NaN|      150.0|           39.0|                     NaN|\n",
      "|       Algeria|     DZ|    DZA|          12|ISO 3166-2:DZ| Africa|Northern Africa|                NaN|        2.0|           15.0|                     NaN|\n",
      "|American Samoa|     AS|    ASM|          16|ISO 3166-2:AS|Oceania|      Polynesia|                NaN|        9.0|           61.0|                     NaN|\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes_iso_df_spark.printSchema()\n",
    "country_codes_iso_df_spark.show(5)\n",
    "country_codes_iso_df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tables\n",
    "country_codes_i94_df_spark_joined = country_codes_i94_df_spark\\\n",
    "                                        .join(country_codes_iso_df_spark, \\\n",
    "                                            (country_codes_i94_df_spark.iso_country_code == \\\n",
    "                                                    country_codes_iso_df_spark.country_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- iso_ccode: string (nullable = true)\n",
      " |-- iso_alpha_2: string (nullable = false)\n",
      " |-- iso_alpha_3: string (nullable = false)\n",
      " |-- iso_3166_2_code: string (nullable = false)\n",
      " |-- iso_country_name: string (nullable = false)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- iso_sub_region: string (nullable = true)\n",
      " |-- iso_region_code: string (nullable = true)\n",
      " |-- iso_sub_region_code: string (nullable = true)\n",
      "\n",
      "+------------+---------------+---------+-----------+-----------+---------------+-------------------+----------+--------------------+---------------+-------------------+\n",
      "|country_code|   country_name|iso_ccode|iso_alpha_2|iso_alpha_3|iso_3166_2_code|   iso_country_name|iso_region|      iso_sub_region|iso_region_code|iso_sub_region_code|\n",
      "+------------+---------------+---------+-----------+-----------+---------------+-------------------+----------+--------------------+---------------+-------------------+\n",
      "|         236|    AFGHANISTAN|        4|         AF|        AFG|  ISO 3166-2:AF|        Afghanistan|      Asia|       Southern Asia|          142.0|               34.0|\n",
      "|         101|        ALBANIA|        8|         AL|        ALB|  ISO 3166-2:AL|            Albania|    Europe|     Southern Europe|          150.0|               39.0|\n",
      "|         316|        ALGERIA|       12|         DZ|        DZA|  ISO 3166-2:DZ|            Algeria|    Africa|     Northern Africa|            2.0|               15.0|\n",
      "|         102|        ANDORRA|       20|         AD|        AND|  ISO 3166-2:AD|            Andorra|    Europe|     Southern Europe|          150.0|               39.0|\n",
      "|         324|         ANGOLA|       24|         AO|        AGO|  ISO 3166-2:AO|             Angola|    Africa|  Sub-Saharan Africa|            2.0|              202.0|\n",
      "|         529|       ANGUILLA|      660|         AI|        AIA|  ISO 3166-2:AI|           Anguilla|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         518|ANTIGUA-BARBUDA|       28|         AG|        ATG|  ISO 3166-2:AG|Antigua and Barbuda|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         687|      ARGENTINA|       32|         AR|        ARG|  ISO 3166-2:AR|          Argentina|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         151|        ARMENIA|       51|         AM|        ARM|  ISO 3166-2:AM|            Armenia|      Asia|        Western Asia|          142.0|              145.0|\n",
      "|         532|          ARUBA|      533|         AW|        ABW|  ISO 3166-2:AW|              Aruba|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         438|      AUSTRALIA|       36|         AU|        AUS|  ISO 3166-2:AU|          Australia|   Oceania|Australia and New...|            9.0|               53.0|\n",
      "|         103|        AUSTRIA|       40|         AT|        AUT|  ISO 3166-2:AT|            Austria|    Europe|      Western Europe|          150.0|              155.0|\n",
      "|         152|     AZERBAIJAN|       31|         AZ|        AZE|  ISO 3166-2:AZ|         Azerbaijan|      Asia|        Western Asia|          142.0|              145.0|\n",
      "|         512|        BAHAMAS|       44|         BS|        BHS|  ISO 3166-2:BS|            Bahamas|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         298|        BAHRAIN|       48|         BH|        BHR|  ISO 3166-2:BH|            Bahrain|      Asia|        Western Asia|          142.0|              145.0|\n",
      "|         274|     BANGLADESH|       50|         BD|        BGD|  ISO 3166-2:BD|         Bangladesh|      Asia|       Southern Asia|          142.0|               34.0|\n",
      "|         513|       BARBADOS|       52|         BB|        BRB|  ISO 3166-2:BB|           Barbados|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         153|        BELARUS|      112|         BY|        BLR|  ISO 3166-2:BY|            Belarus|    Europe|      Eastern Europe|          150.0|              151.0|\n",
      "|         104|        BELGIUM|       56|         BE|        BEL|  ISO 3166-2:BE|            Belgium|    Europe|      Western Europe|          150.0|              155.0|\n",
      "|         581|         BELIZE|       84|         BZ|        BLZ|  ISO 3166-2:BZ|             Belize|  Americas|Latin America and...|           19.0|              419.0|\n",
      "+------------+---------------+---------+-----------+-----------+---------------+-------------------+----------+--------------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table\n",
    "country_codes_i94_df_spark_joined.createOrReplaceTempView(\"countries_table_DF\")\n",
    "countries_table = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT i94_cit          AS country_code,\n",
    "                        i94_country_name AS country_name,\n",
    "                        iso_country_code AS iso_ccode,\n",
    "                        alpha_2          AS iso_alpha_2,\n",
    "                        alpha_3          AS iso_alpha_3,\n",
    "                        iso_3166_2       AS iso_3166_2_code,\n",
    "                        name             AS iso_country_name,\n",
    "                        region           AS iso_region,\n",
    "                        sub_region       AS iso_sub_region,\n",
    "                        region_code      AS iso_region_code,\n",
    "                        sub_region_code  AS iso_sub_region_code\n",
    "        FROM countries_table_DF          AS countries\n",
    "        ORDER BY country_name\n",
    "    \n",
    "\"\"\")\n",
    "countries_table.printSchema()\n",
    "countries_table.show(20)\n",
    "countries_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/countries_table.parquet_2019-08-21-08-33-50-378418\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write countries_table to parquet file:\n",
    "countries_table_path = output_data + \"countries_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {countries_table_path}\")\n",
    "countries_table.write.mode(\"overwrite\").parquet(countries_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "countries_table_df = spark.read.parquet(countries_table_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Create airports table + write to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- i94_airport_name: string (nullable = true)\n",
      " |-- i94_airport_state: string (nullable = true)\n",
      "\n",
      "+-------+--------------------------+-----------------+\n",
      "|i94port|i94_airport_name          |i94_airport_state|\n",
      "+-------+--------------------------+-----------------+\n",
      "|ELM    |REGIONAL ARPT - HORSEHEAD |NY               |\n",
      "|ROC    |ROCHESTER                 |NY               |\n",
      "|ROU    |ROUSES POINT              |NY               |\n",
      "|SWF    |STEWART - ORANGE CNTY     |NY               |\n",
      "|SYR    |SYRACUSE                  |NY               |\n",
      "|THO    |THOUSAND ISLAND BRIDGE    |NY               |\n",
      "|TRO    |TROUT RIVER               |NY               |\n",
      "|WAT    |WATERTOWN                 |NY               |\n",
      "|HPN    |WESTCHESTER - WHITE PLAINS|NY               |\n",
      "|WRB    |WHIRLPOOL BRIDGE          |NY               |\n",
      "|YOU    |YOUNGSTOWN                |NY               |\n",
      "|AKR    |AKRON                     |OH               |\n",
      "|ATB    |ASHTABULA                 |OH               |\n",
      "|CIN    |CINCINNATI                |OH               |\n",
      "|CLE    |CLEVELAND                 |OH               |\n",
      "+-------+--------------------------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#airport_codes_i94_df_spark.printSchema()\n",
    "#airport_codes_i94_df_spark.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- airport_name: string (nullable = true)\n",
      " |-- airport_state: string (nullable = true)\n",
      "\n",
      "+----------+--------------------+-------------+\n",
      "|airport_id|        airport_name|airport_state|\n",
      "+----------+--------------------+-------------+\n",
      "|       ABE|            ABERDEEN|           WA|\n",
      "|       ADS|ADDISON AIRPORT- ...|           TX|\n",
      "|       AGA|               AGANA|           GU|\n",
      "|       AGU|           AGUADILLA|           PR|\n",
      "|       BOI|AIR TERM. (GOWEN ...|           ID|\n",
      "|       AKR|               AKRON|           OH|\n",
      "|       CAK|               AKRON|           OH|\n",
      "|       ALA|          ALAMAGORDO|     NM (BPS)|\n",
      "|       ALB|              ALBANY|           NY|\n",
      "|       CHO|ALBEMARLE CHARLOT...|           VA|\n",
      "|       ABQ|         ALBUQUERQUE|           NM|\n",
      "|       ABG|              ALBURG|           VT|\n",
      "|       ABS|      ALBURG SPRINGS|           VT|\n",
      "|       ALC|               ALCAN|           AK|\n",
      "|       AXB|      ALEXANDRIA BAY|           NY|\n",
      "|       AGM|              ALGOMA|           WI|\n",
      "|       AGN|             ALGONAC|           MI|\n",
      "|       ALP|              ALPENA|           MI|\n",
      "|       AMB|             AMBROSE|           ND|\n",
      "|       ADT|         AMISTAD DAM|           TX|\n",
      "+----------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table\n",
    "airport_codes_i94_df_spark.createOrReplaceTempView(\"airports_table_DF\")\n",
    "airports_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT  i94_port          AS airport_id, \n",
    "                     i94_airport_name  AS airport_name,\n",
    "                     i94_airport_state AS airport_state\n",
    "    FROM airports_table_DF             AS airports\n",
    "    ORDER BY airport_name\n",
    "\"\"\")\n",
    "\n",
    "airports_table.printSchema()\n",
    "airports_table.show(20)\n",
    "airports_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/airports_table.parquet_2019-08-21-08-33-50-378418\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write airports_table to parquet file:\n",
    "airports_table_path = output_data + \"airports_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {airports_table_path}\")\n",
    "airports_table.write.mode(\"overwrite\").parquet(airports_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "airports_table_df = spark.read.parquet(airports_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Create time table + write to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column creation DONE.\n"
     ]
    }
   ],
   "source": [
    "@udf(t.TimestampType())\n",
    "def get_timestamp (arrdate):\n",
    "    arrdate_int = int(arrdate)\n",
    "    return (datetime(1960,1,1) + timedelta(days=arrdate_int))\n",
    "    \n",
    "i94_df_spark_clean = i94_df_spark_clean.withColumn(\"arrival_time\", get_timestamp(i94_df_spark.arrdate))\n",
    "print(\"New column creation DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i94_df_spark_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival_ts: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_df_spark_clean.createOrReplaceTempView(\"time_table_DF\")\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT  arrival_time           AS arrival_ts, \n",
    "                     hour(arrival_time)       AS hour, \n",
    "                     day(arrival_time)        AS day, \n",
    "                     weekofyear(arrival_time) AS week,\n",
    "                     month(arrival_time)      AS month,\n",
    "                     year(arrival_time)       AS year,\n",
    "                     dayofweek(arrival_time)  AS weekday\n",
    "    FROM time_table_DF\n",
    "\"\"\")\n",
    "time_table.printSchema()\n",
    "#time_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/time_table.parquet_2019-08-21-08-33-50-378418\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write time_table to parquet file:\n",
    "time_table_path = output_data + \"time_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {time_table_path}\")\n",
    "time_table.write.mode(\"append\").partitionBy(\"year\", \"month\")\\\n",
    "                                  .parquet(time_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "time_table_df = spark.read.parquet(time_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Create immigrations table + write to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i94_df_spark_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countries_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"depdate_ts\" column at this phase (before joining tables)\n",
    "# Create \"immigration_id\" column at this phase (before joining tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94_df_spark_joined = i94_df_spark_clean.join(country_codes_i94_df_spark, \\\n",
    "                                              (i94_df_spark_clean.i94cit == country_codes_i94_df_spark.i94_cit))\\\n",
    "                                        .join(airport_codes_i94_df_spark, \\\n",
    "                                              (i94_df_spark_clean.i94port == airport_codes_i94_df_spark.i94_port))\\\n",
    "                                        .join(time_table_df, \\\n",
    "                                                            i94_df_spark_clean.arrival_time == \\\n",
    "                                                            time_table_df.arrival_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = false)\n",
      " |-- i94addr: string (nullable = false)\n",
      " |-- depdate: double (nullable = false)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = false)\n",
      " |-- count: double (nullable = false)\n",
      " |-- dtadfile: string (nullable = false)\n",
      " |-- visapost: string (nullable = false)\n",
      " |-- occup: string (nullable = false)\n",
      " |-- entdepa: string (nullable = false)\n",
      " |-- entdepd: string (nullable = false)\n",
      " |-- entdepu: string (nullable = false)\n",
      " |-- matflag: string (nullable = false)\n",
      " |-- biryear: double (nullable = false)\n",
      " |-- dtaddto: string (nullable = false)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- insnum: string (nullable = false)\n",
      " |-- airline: string (nullable = false)\n",
      " |-- admnum: double (nullable = false)\n",
      " |-- fltno: string (nullable = false)\n",
      " |-- visatype: string (nullable = false)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- i94_cit: string (nullable = true)\n",
      " |-- i94_country_name: string (nullable = true)\n",
      " |-- iso_country_code: string (nullable = true)\n",
      " |-- i94_port: string (nullable = true)\n",
      " |-- i94_airport_name: string (nullable = true)\n",
      " |-- i94_airport_state: string (nullable = true)\n",
      " |-- arrival_ts: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_df_spark_joined.printSchema()\n",
    "#i94_df_spark_joined.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column creation DONE.\n"
     ]
    }
   ],
   "source": [
    "i94_df_spark_joined = i94_df_spark_joined.withColumn(\"immigration_id\", monotonically_increasing_id())\n",
    "print(\"New column creation DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i94_df_spark_joined.printSchema()\n",
    "#i94_df_spark_joined.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i94_df_spark_joined.createOrReplaceTempView(\"immigrants_table_DF\")\n",
    "#immigrants_table_check = spark.sql(\"\"\"\n",
    "#    SELECT  cicid, i94yr, i94mon, i94cit, i94res, i94port, arrdate, \\\n",
    "#            i94mode, airline, fltno, depdate, i94bir, i94visa, gender,  \\\n",
    "#            visatype, admnum\n",
    "#    FROM immigrants_table_DF\n",
    "#    WHERE   cicid == null OR arrdate == null OR i94port == null \\\n",
    "#            OR fltno == null OR i94mode == null OR admnum == null \\\n",
    "#            OR gender == null OR admnum == null OR depdate == null\n",
    "#    ORDER BY arrdate\n",
    "#\"\"\")\n",
    "#immigrants_table_check.printSchema()\n",
    "#immigrants_table_check.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column creation DONE.\n"
     ]
    }
   ],
   "source": [
    "@udf(t.TimestampType())\n",
    "def get_timestamp2 (depdate):\n",
    "    if depdate == \"null\":\n",
    "        depdate_int = 0\n",
    "    else:\n",
    "        depdate_int = int(depdate)\n",
    "    return (datetime(1960,1,1) + timedelta(days=depdate_int))\n",
    "\n",
    "i94_df_spark_joined = i94_df_spark_joined.withColumn(\"departure_date\", get_timestamp2(i94_df_spark_joined.depdate))\n",
    "print(\"New column creation DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i94_df_spark_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- immigration_id: long (nullable = false)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- admission_nbr: double (nullable = false)\n",
      " |-- arrival_mode: double (nullable = false)\n",
      " |-- departure_date: timestamp (nullable = true)\n",
      " |-- airline: string (nullable = false)\n",
      " |-- flight_nbr: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i94_df_spark_joined.createOrReplaceTempView(\"immigrations_table_DF\")\n",
    "immigrations_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT  immigration_id AS immigration_id, \n",
    "                     arrival_time   AS arrival_time,\n",
    "                     year           AS arrival_year,\n",
    "                     month          AS arrival_month,\n",
    "                     i94_port       AS airport_id,\n",
    "                     i94_cit        AS country_code,\n",
    "                     admnum         AS admission_nbr,\n",
    "                     i94mode        AS arrival_mode,\n",
    "                     departure_date AS departure_date,\n",
    "                     airline        AS airline,\n",
    "                     fltno          AS flight_nbr\n",
    "                    \n",
    "    FROM immigrations_table_DF immigrants\n",
    "    ORDER BY arrival_time\n",
    "\"\"\")\n",
    "immigrations_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+------------+-------------+----------+------------+-------------+------------+-------------------+-------+----------+\n",
      "|immigration_id|arrival_time       |arrival_year|arrival_month|airport_id|country_code|admission_nbr|arrival_mode|departure_date     |airline|flight_nbr|\n",
      "+--------------+-------------------+------------+-------------+----------+------------+-------------+------------+-------------------+-------+----------+\n",
      "|11714         |2016-01-01 00:00:00|2016        |1            |SPM       |135         |2.85902085E8 |1.0         |1960-01-01 00:00:00|BA     |243       |\n",
      "|33049         |2016-01-01 00:00:00|2016        |1            |NYC       |245         |2.85799385E8 |1.0         |2016-03-19 00:00:00|KE     |85        |\n",
      "|21283         |2016-01-01 00:00:00|2016        |1            |SFR       |213         |2.83135585E8 |1.0         |2016-01-03 00:00:00|AI     |173       |\n",
      "|3945          |2016-01-01 00:00:00|2016        |1            |SPM       |116         |2.85342985E8 |1.0         |1960-01-01 00:00:00|BA     |243       |\n",
      "|21288         |2016-01-01 00:00:00|2016        |1            |SFR       |213         |2.84495985E8 |1.0         |1960-01-01 00:00:00|EK     |225       |\n",
      "+--------------+-------------------+------------+-------------+----------+------------+-------------+------------+-------------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#immigrations_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: data/output_data/immigrations_table.parquet_2019-08-21-08-33-50-378418\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write immigrants_table to parquet file:\n",
    "immigrations_table_path = output_data + \"immigrations_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {immigrations_table_path}\")\n",
    "immigrations_table.write.mode(\"append\").partitionBy(\"arrival_year\", \"arrival_month\")\\\n",
    "                                          .parquet(immigrations_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Read parquet file back to Spark:\n",
    "immigrations_table_df = spark.read.parquet(immigrations_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "_Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:_\n",
    " * _Integrity constraints on the relational database (e.g., unique key, data type, etc.)_\n",
    " * _Unit tests for the scripts to ensure they are doing the right thing_\n",
    " * _Source/Count checks to ensure completeness_\n",
    " \n",
    "_Run Quality Checks_\n",
    "\n",
    "**Data quality checks:**\n",
    " * Check that all primary and secondary keys in star schema dimension and fact tables have values.\n",
    " * Check that all tables have more than 0 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_ts = start_time\n",
    "results = { \"round_ts\": round_ts,\n",
    "            \"admissions_count\": 0,\n",
    "            \"admissions\": \"\",\n",
    "            \"countries_count\": 0,\n",
    "            \"countries\": \"\",\n",
    "            \"airports_count\": 0,\n",
    "            \"airports\": \"\",\n",
    "            \"time_count\": 0,\n",
    "            \"time\": \"\",\n",
    "            \"immigrations_count\": 0,\n",
    "            \"immigrations\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Quality checks for admissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that key fields have valid values (no nulls or empty)\n",
    "admissions_table_df.createOrReplaceTempView(\"admissions_table_DF\")\n",
    "admissions_table_check1 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM admissions_table_DF\n",
    "    WHERE   admission_nbr IS NULL OR admission_nbr == \"\" OR \n",
    "            country_code IS NULL OR country_code == \"\" \n",
    "\"\"\")\n",
    "admissions_table_check1.show(1)\n",
    "admissions_table_check1.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2833480|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2833480"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that table has > 0 rows\n",
    "admissions_table_df.createOrReplaceTempView(\"admissions_table_DF\")\n",
    "admissions_table_check2 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM admissions_table_DF\n",
    "\"\"\")\n",
    "admissions_table_check2.show(1)\n",
    "admissions_table_check2.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS: {'round_ts': '2019-08-21-08-33-50-378418', 'admissions_count': 2833480, 'admissions': 'OK', 'countries_count': 0, 'countries': '', 'airports_count': 0, 'airports': '', 'time_count': 0, 'time': '', 'immigrations_count': 0, 'immigrations': ''}\n"
     ]
    }
   ],
   "source": [
    "if admissions_table_check1.collect()[0][0] > 0 & admissions_table_check2.collect()[0][0] < 1:\n",
    "    results['admissions_count'] = admissions_table_check2.collect()[0][0]\n",
    "    results['admissions'] = \"NOK\"\n",
    "else:\n",
    "    results['admissions_count'] = admissions_table_check2.collect()[0][0]\n",
    "    results['admissions'] = \"OK\"\n",
    "\n",
    "print(f\"RESULTS: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Quality checks for countries table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that key fields have valid values (no nulls or empty)\n",
    "countries_table_df.createOrReplaceTempView(\"countries_table_DF\")\n",
    "countries_table_check1 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM countries_table_DF\n",
    "    WHERE   country_code IS NULL OR country_code == \"\"\n",
    "\"\"\")\n",
    "countries_table_check1.show(1)\n",
    "countries_table_check1.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     229|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that table has > 0 rows\n",
    "countries_table_df.createOrReplaceTempView(\"countries_table_DF\")\n",
    "countries_table_check2 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM countries_table_DF\n",
    "\"\"\")\n",
    "countries_table_check2.show(1)\n",
    "countries_table_check2.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS: {'round_ts': '2019-08-21-08-33-50-378418', 'admissions_count': 2833480, 'admissions': 'OK', 'countries_count': 229, 'countries': 'OK', 'airports_count': 0, 'airports': '', 'time_count': 0, 'time': '', 'immigrations_count': 0, 'immigrations': ''}\n"
     ]
    }
   ],
   "source": [
    "if countries_table_check1.collect()[0][0] > 0 & countries_table_check2.collect()[0][0] < 1:\n",
    "    results['countries_count'] = countries_table_check2.collect()[0][0]\n",
    "    results['countries'] = \"NOK\"\n",
    "else:\n",
    "    results['countries_count'] = countries_table_check2.collect()[0][0]\n",
    "    results['countries'] = \"OK\"\n",
    "\n",
    "print(f\"RESULTS: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Quality checks for airports table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     229|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that key fields have valid values (no nulls or empty)\n",
    "airports_table_df.createOrReplaceTempView(\"airports_table_DF\")\n",
    "airports_table_check1 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM airports_table_DF\n",
    "    WHERE   airport_id IS NULL OR airport_id == \"\" OR\n",
    "            airport_name IS NULL OR airport_name == \"\"\n",
    "\"\"\")\n",
    "countries_table_check2.show(1)\n",
    "countries_table_check2.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     660|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that table has > 0 rows\n",
    "airports_table_df.createOrReplaceTempView(\"airports_table_DF\")\n",
    "airports_table_check2 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM airports_table_DF\n",
    "\"\"\")\n",
    "airports_table_check2.show(1)\n",
    "airports_table_check2.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS: {'round_ts': '2019-08-21-08-33-50-378418', 'admissions_count': 2833480, 'admissions': 'OK', 'countries_count': 229, 'countries': 'OK', 'airports_count': 660, 'airports': 'OK', 'time_count': 0, 'time': '', 'immigrations_count': 0, 'immigrations': ''}\n"
     ]
    }
   ],
   "source": [
    "if airports_table_check1.collect()[0][0] > 0 & airports_table_check2.collect()[0][0] < 1:\n",
    "    results['airports_count'] = airports_table_check2.collect()[0][0]\n",
    "    results['airports'] = \"NOK\"\n",
    "else:\n",
    "    results['airports_count'] = airports_table_check2.collect()[0][0]\n",
    "    results['airports'] = \"OK\"\n",
    "\n",
    "print(f\"RESULTS: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Quality checks for time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that key fields have valid values (no nulls or empty)\n",
    "time_table_df.createOrReplaceTempView(\"time_table_DF\")\n",
    "time_table_check1 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM time_table_DF\n",
    "    WHERE   arrival_ts IS NULL OR arrival_ts == \"\"\n",
    "\"\"\")\n",
    "time_table_check1.show(1)\n",
    "time_table_check1.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      31|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that table has > 0 rows\n",
    "time_table_df.createOrReplaceTempView(\"time_table_DF\")\n",
    "time_table_check2 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM time_table_DF\n",
    "\"\"\")\n",
    "time_table_check2.show(1)\n",
    "time_table_check2.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS: {'round_ts': '2019-08-21-08-33-50-378418', 'admissions_count': 2833480, 'admissions': 'OK', 'countries_count': 229, 'countries': 'OK', 'airports_count': 660, 'airports': 'OK', 'time_count': 31, 'time': 'OK', 'immigrations_count': 0, 'immigrations': ''}\n"
     ]
    }
   ],
   "source": [
    "if time_table_check1.collect()[0][0] > 0 & time_table_check2.collect()[0][0] < 1:\n",
    "    results['time_count'] = time_table_check2.collect()[0][0]\n",
    "    results['time'] = \"NOK\"\n",
    "else:\n",
    "    results['time_count'] = time_table_check2.collect()[0][0]\n",
    "    results['time'] = \"OK\"\n",
    "\n",
    "print(f\"RESULTS: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Quality checks for immigrations table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#immigrations_table_path = \"data/output_data/immigrations_table.parquet_2019-08-15-12-22-26-417652\"\n",
    "#print(f\"OUTPUT: {immigrations_table_path}\")\n",
    "#immigrations_table_df = spark.read.parquet(immigrations_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2450639"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigrations_table_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that key fields have valid values (no nulls or empty)\n",
    "immigrations_table_df.createOrReplaceTempView(\"immigrations_table_DF\")\n",
    "immigrations_table_check1 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM immigrations_table_DF\n",
    "        WHERE   immigration_id IS NULL OR immigration_id == \"\" OR\n",
    "                arrival_time IS NULL OR arrival_time == \"\" OR\n",
    "                arrival_year IS NULL OR arrival_year == \"\" OR\n",
    "                arrival_month IS NULL OR arrival_month == \"\" OR\n",
    "                airport_id IS NULL OR airport_id == \"\" OR\n",
    "                country_code IS NULL OR country_code == \"\" OR\n",
    "                admission_nbr IS NULL OR admission_nbr == \"\"\n",
    "\"\"\")\n",
    "immigrations_table_check1.show(1)\n",
    "immigrations_table_check1.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2450639|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2450639"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that table has > 0 rows\n",
    "immigrations_table_df.createOrReplaceTempView(\"immigrations_table_DF\")\n",
    "immigrations_table_check2 = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM immigrations_table_DF\n",
    "\"\"\")\n",
    "immigrations_table_check2.show(1)\n",
    "immigrations_table_check2.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS: {'round_ts': '2019-08-21-08-33-50-378418', 'admissions_count': 2833480, 'admissions': 'OK', 'countries_count': 229, 'countries': 'OK', 'airports_count': 660, 'airports': 'OK', 'time_count': 31, 'time': 'OK', 'immigrations_count': 2450639, 'immigrations': 'OK'}\n"
     ]
    }
   ],
   "source": [
    "if immigrations_table_check1.collect()[0][0] > 0 & immigrations_table_check2.collect()[0][0] < 1:\n",
    "    results['immigrations_count'] = immigrations_table_check2.collect()[0][0]\n",
    "    results['immigrations'] = \"NOK\"\n",
    "else:\n",
    "    results['immigrations_count'] = immigrations_table_check2.collect()[0][0]\n",
    "    results['immigrations'] = \"OK\"\n",
    "\n",
    "print(f\"RESULTS: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "_Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file._\n",
    "\n",
    "Data Dictionary for the project is described in **data_dictionary.json** file stored in the project root directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* _Clearly state the rationale for the choice of tools and technologies for the project._\n",
    "* _Propose how often the data should be updated and why._\n",
    "* _Write a description of how you would approach the problem differently under the following scenarios:_\n",
    " * _The data was increased by 100x._\n",
    " * _The data populates a dashboard that must be updated on a daily basis by 7am every day._\n",
    " * _The database needed to be accessed by 100+ people._\n",
    " \n",
    "**Rationale for the tools selection:**\n",
    "* Python, Pandas and Spark were natural choises to process project's input data since it contains all necessary (and easy to use) libraries to read, clean, process, and form DB tables.\n",
    "* Since the data set was still limited, local and server storage was used in storing, reading, writing the input and output data. \n",
    "* Input data could have been stored in AWS without big problems (excluded in this project). \n",
    "* Output data could have been easily written to AWS after processing (excluded in this project). Experiences have shown that it's better to write parquet files locally first and only after that write them to cloud storage (as a bulk oparation) to avoid delays and extra costs caused by AWS S3.\n",
    "\n",
    "**How often ETL script should be run:**\n",
    "* ETL script should be run monthly basis (assuming that new I94 data is available once per month).\n",
    "\n",
    "**Other scenarions (what to consider in them):**\n",
    "* Data is 100x: \n",
    "    * Input data should be stoted in cloud storage e.g. AWS S3\n",
    "    * Clustered Spark should be used to enable parallel processing of the data.\n",
    "    * Clustered Cloud DB e.g. AWS Redshift should be used to store the data during the processing (staging and final tables).  \n",
    "    * Output data (parquet files) should be stored to Cloud storage e.g. AWS S3 for easy access or to a Cloud DB for further analysis. AWS Redshift is very expensive for storing the data, so maybe some SQL DB (e.g. AWS RDS) should be used. \n",
    "    \n",
    "* Data is used in dashboard and updated every day 07:00AM:\n",
    "    * ETl script should be refactored to process only the changed inout information instead of processing all the inout files as it does now to minimise the used time and comouting resources.\n",
    "    * Output data should be stored and updated in a Cloud DB (e.g. AWS RDS) to make it available all times for the dashboard.\n",
    "    * Possibly this \"always available\" DB (serving the dashboard) would contain a latest sub-set of all available data to make it fast perfoming and easier to manage.\n",
    "\n",
    "* DB is accessed by 100+ people:\n",
    "    * Output data should be stored in a Cloud DB (e.g. AWS RDS) to make it \"always available\" for further analysis. Tools should be provided for the end-users to access the output DB. \n",
    "    * Potentially, some new tables could be created to serve the most used queries better.\n",
    "\n",
    "**Potential further work:** \n",
    "* ETL pipeline script could be re-factored\n",
    "    * make it more modular (split functions to separate files/classes)\n",
    "    * combine functions to have fewer, more general purpose functions instead of several specific function per ETL steps \n",
    "    \n",
    "* IATA airport data could be (semi-manually) mapped to I94 airport data to add more value for the analysis and enable further data merges.\n",
    "\n",
    "* Other data e.g. daily weather data could be combined as inout data to provide insights about the weather immigrants experienced when they entered US. \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
